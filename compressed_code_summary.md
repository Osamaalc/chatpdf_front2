Project Root: /home/drcinco/Pictures/mini_RAG
Project Structure:
```
.
|-- .gitignore
|-- controllers
    |-- BaseController.py
    |-- DataController.py
    |-- NLPController.py
    |-- OCRController.py
    |-- ProcessController.py
    |-- ProjectController.py
    |-- SummarizationController.py
    |-- __init__.py
|-- fix_app_state.sh
|-- helpers
    |-- __init__.py
    |-- config.py
    |-- pdf_ocr_processor.py
    |-- pdf_parser.py
    |-- summarization_text_splitter.py
    |-- text_cleaner.py
|-- main.py
|-- mk.py
|-- models
    |-- AssetModel.py
    |-- AssetSettingsModel.py
    |-- BaseDataModel.py
    |-- ChunkModel.py
    |-- ProjectModel.py
    |-- SummarizationChunkModel.py
    |-- __init__.py
    |-- db_schemes
        |-- AssetSettings.py
        |-- Project.py
        |-- __init__.py
        |-- asset.py
        |-- data_chunk.py
    |-- enums
        |-- AssetTypeEnum.py
        |-- DataBaseEnum.py
        |-- ProcessingEnum.py
        |-- ResponseEnums.py
        |-- __init__.py
|-- rag_info.txt
|-- requirements.txt
|-- routes
    |-- __init__.py
    |-- asset_settings_router.py
    |-- base.py
    |-- data.py
    |-- file.py
    |-- integrated.py
    |-- nlp.py
    |-- schems
        |-- __init__.py
        |-- data.py
        |-- nlp.py
        |-- summarization.py
    |-- summarization.py
|-- stores
    |-- Summarization
        |-- SummarizationEnums.py
        |-- SummarizationInterface.py
        |-- SummarizationProviderFactory.py
        |-- SummaryMerger.py
        |-- SummaryQualityChecker.py
        |-- providers
            |-- OpenAISummarizationProvider.py
            |-- __init__.py
    |-- llm
        |-- LLMEnums.py
        |-- LLMInterface.py
        |-- LLMProviderFactory.py
        |-- __init__.py
        |-- providers
            |-- CoHereProvider.py
            |-- DeepseekProvider.py
            |-- GeminiProvider.py
            |-- OpenAIProvider.py
            |-- __init__.py
    |-- templates
        |-- __init__.py
        |-- locales
            |-- __init__.py
            |-- ar
                |-- __init__.py
                |-- rag.py
            |-- en
                |-- __init__.py
                |-- rag.py
        |-- summarization_templates
            |-- chunk_prompt_template.py
            |-- merge_prompt_template.py
        |-- template_parser.py
    |-- vectordb
        |-- VectorDBEnums.py
        |-- VectorDBInterfase.py
        |-- VectorDBProviderFactory.py
        |-- __init__.py
        |-- providers
            |-- MilvusDBProvider.py
            |-- QdrantDBProvider.py
            |-- WeaviateDBProvider.py
            |-- __init__.py
|-- utils
|-- websocket_handlers.py

```

---

## File: .gitignore

Summary:
```markdown
This code is a `.gitignore` file template for Python projects. It specifies a list of files and directories that should be ignored by Git, preventing them from being tracked in version control. This is essential for maintaining a clean repository by excluding unnecessary files that are generated during development, build processes, or by various tools and environments.

### Key Sections:

1. **Byte-compiled/optimized files**: Ignores Python bytecode files (`*.pyc`, `*.pyo`, etc.) and the `__pycache__` directory.

2. **C extensions**: Ignores shared object files (`*.so`).

3. **Distribution/packaging**: Excludes directories and files related to Python packaging and distribution, such as `build/`, `dist/`, `*.egg-info/`, and `MANIFEST`.

4. **PyInstaller**: Ignores files related to PyInstaller, such as `*.spec` and `*.manifest`.

5. **Installer logs**: Excludes log files generated by pip (`pip-log.txt`).

6. **Unit test/coverage reports**: Ignores directories and files related to testing and coverage, such as `.tox/`, `.coverage`, and `*.xml`.

7. **Translations**: Ignores compiled translation files (`*.mo`).

8. **Framework-specific files**: Excludes files specific to frameworks like Django (`*.log`, `db.sqlite3`) and Flask (`instance/`).

9. **Documentation**: Ignores build directories for documentation tools like Sphinx (`docs/_build/`).

10. **Environment files**: Excludes virtual environment directories (`env/`, `venv/`) and configuration files (`.env`).

11. **IDE/project settings**: Ignores settings for various IDEs and project management tools, such as PyCharm (`.idea/`), Spyder (`.spyderproject`), and Rope (`.ropeproject`).

12. **Static analysis/type checking**: Excludes caches and configuration files for tools like mypy (`.mypy_cache/`), Pyre (`.pyre/`), and pytype (`.pytype/`).

13. **Miscellaneous**: Ignores other tool-specific files and directories, such as Cython debug symbols (`cython_debug/`) and PyPI configuration (`.pypirc`).

### Notes:

- This template is comprehensive and covers a wide range of tools and scenarios commonly encountered in Python development.
- Some sections provide comments on whether certain files (like lock files) should be included in version control, depending on the project type and collaboration needs.
- The template is adaptable; users can uncomment or modify sections based on their specific project requirements.
```
---

## File: controllers/BaseController.py

Summary:
```markdown
### Summary

The code defines a `BaseController` class that provides basic configuration and utility functions for managing file paths and generating random strings. It relies on a settings module to initialize application settings and defines methods for generating random strings and managing database paths.

### Classes

#### `BaseController`

- **Purpose**: Initializes settings and manages paths for files and databases. Provides utility functions for generating random strings and ensuring database directories exist.

- **Attributes**:
  - `app_settings`: Stores application settings retrieved from the `get_settings()` function.
  - `base_dir`: The base directory of the current file.
  - `file_dir`: Path to the directory where files are stored (`assets/files`).
  - `database_dir`: Path to the directory where databases are stored (`assets/database`).

- **Methods**:
  - `__init__()`: Initializes the controller by setting up application settings and defining base directories for files and databases.
  - `genarate_random_string(length: int = 12) -> str`: Generates a random string of a specified length (default is 12) using lowercase letters and digits.
  - `get_database_path(db_name: str) -> str`: Constructs the full path for a given database name. Creates the directory if it does not exist and returns the path.

### Dependencies

- **Modules**:
  - `os`: Used for path manipulations and checking/creating directories.
  - `random`: Utilized for generating random choices of characters.
  - `string`: Provides string constants for generating random strings.
  - `helpers.config`: Assumed to contain the `get_settings` function and `Settings` class for application configuration.

### Notes

- The `get_settings()` function and `Settings` class are not defined within this code snippet but are imported from `helpers.config`. They are assumed to handle application-specific configuration settings.
- The method `genarate_random_string` contains a typo in its name; it should be `generate_random_string` for consistency and correctness.
- The code includes comments in Arabic, indicating the purpose of each section, which may be relevant for developers familiar with the language.
```
---

## File: controllers/DataController.py

Summary:
```markdown
### Code Summary

The provided code defines a `DataController` class that inherits from a `BaseController`. This class is designed to handle file upload operations, including validating the uploaded files and generating unique file paths for storage.

### Class: `DataController`

- **Inheritance**: Inherits from `BaseController`.

- **Attributes**:
  - `size_scale`: An integer representing the size of one megabyte in bytes (1 MB = 1048576 bytes).

- **Methods**:
  - `__init__()`: Initializes the `DataController` instance, setting the `size_scale` attribute and calling the parent class's constructor.

  - `validate_uploaded_file(file: UploadFile)`: Validates the uploaded file based on its type and size.
    - **Parameters**:
      - `file`: An instance of `UploadFile` from FastAPI, representing the uploaded file.
    - **Returns**: A tuple containing a boolean indicating the validation result and a response signal message.
    - **Validation Criteria**:
      - Checks if the file's content type is allowed.
      - Checks if the file's size exceeds the maximum allowed size.

  - `generate_unique_filepath(orig_file_name: str)`: Generates a unique file path for the uploaded file.
    - **Parameters**:
      - `orig_file_name`: A string representing the original name of the file.
    - **Returns**: A tuple containing the new unique file path and the file identifier.
    - **Process**:
      - Generates a random string to ensure uniqueness.
      - Cleans the original file name to remove unwanted characters.
      - Constructs a new file path and ensures it is unique by checking if it already exists.

  - `get_clean_file_name(orig_file_name: str)`: Cleans the original file name by removing unwanted characters and replacing spaces with underscores.
    - **Parameters**:
      - `orig_file_name`: A string representing the original name of the file.
    - **Returns**: A cleaned version of the file name.

### Notes

- The `DataController` class relies on certain settings from `app_settings` for file type and size validation.
- The `ResponseSignal` is used to provide standardized response messages for file validation outcomes.
- The `generate_unique_filepath` method ensures that file paths are unique by appending a random string to the cleaned file name and checking for existing paths.
- The code uses regular expressions to clean file names, allowing only word characters and periods.
```
---

## File: controllers/NLPController.py

Summary:
```markdown
The `NLPController` class is a component of a larger system designed to manage and interact with a vector database for natural language processing tasks. It extends the `BaseController` class and utilizes various clients for database operations, embedding generation, and text generation. Below is a summary of its key components and functions:

### Class: `NLPController`
- **Purpose**: Manages vector database operations and facilitates natural language processing tasks such as embedding text, searching, and generating responses using a language model.
- **Dependencies**: Requires clients for vector database (`vectordb_client`), general database (`db_client`), text embedding (`embedding_client`), text generation (`generation_client`), and template parsing (`template_parser`).

### Key Methods:

- **`__init__`**: Initializes the controller with required clients and configuration settings.
  
- **`create_collection_name(asset_id: str)`**: Generates a collection name for the vector database based on the asset ID.

- **`reset_vector_db_collection(asset: Asset)`**: Resets the vector database collection associated with a given asset.

- **`get_vector_db_collection_info(asset: Asset)`**: Retrieves configuration information for a vector database collection associated with an asset.

- **`index_info_vector_db(asset: Asset, chunks: List[DataChunk], chunks_ids: List[int], do_rest: bool = False)`**: 
  - Embeds text chunks and inserts them into the vector database.
  - Parameters:
    - `asset`: The asset object.
    - `chunks`: List of text chunks to be embedded and stored.
    - `chunks_ids`: List of IDs corresponding to each chunk.
    - `do_rest`: Boolean flag to reset the collection if it exists.

- **`search_vector_db_collection(asset: Asset, text: str, limit: int = 10)`**: 
  - Performs a semantic search in the vector database using a text query.
  - Parameters:
    - `asset`: The asset object.
    - `text`: The query text.
    - `limit`: Maximum number of results to return.

- **`answer_rag_question(asset: Asset, query: str, limit: int = 10)`**: 
  - Asynchronously answers a question using retrieved documents and a language model.
  - Parameters:
    - `asset`: The asset object.
    - `query`: The user's query.
    - `limit`: Maximum number of documents to retrieve for context.
  - Steps include loading chat history, retrieving documents, building prompts, generating answers, and storing the interaction.

### Notes:
- The class uses a vector database for storing and retrieving text embeddings.
- It supports semantic search and question answering using a language model.
- It handles configuration and model settings dynamically based on asset-specific settings.
- The class logs operations and errors for monitoring and debugging purposes.
```
---

## File: controllers/OCRController.py

Summary:
```markdown
The `OCRController` class is designed to manage OCR (Optical Character Recognition) operations on PDF files using the PDF24 service. It extends from a `BaseController` and provides several methods to facilitate OCR processing, status checking, and cleanup of OCR-generated files.

### Class: `OCRController`

- **Initialization (`__init__`)**: 
  - Sets up logging and initializes a `PDF24OCRClient` for OCR operations.
  - Ensures an OCR directory exists within the file directory for storing processed files.

### Methods:

- **`process_pdf_with_ocr(input_file_path: str, output_file_path: str, custom_config: Optional[Dict[str, Any]] = None) -> bool`**:
  - Processes a PDF file with OCR using the PDF24 service.
  - **Parameters**:
    - `input_file_path`: Path to the input PDF file.
    - `output_file_path`: Path to save the OCR-processed file.
    - `custom_config`: Optional dictionary for custom OCR configurations.
  - **Returns**: `True` if processing is successful, `False` otherwise.

- **`is_ocr_needed(file_path: str) -> bool`**:
  - Determines if a PDF file requires OCR processing.
  - **Parameters**:
    - `file_path`: Path to the PDF file.
  - **Returns**: `True` if OCR is needed, currently set to process all PDFs.

- **`batch_process_ocr(file_paths: list, output_dir: str) -> Dict[str, bool]`**:
  - Processes multiple PDF files with OCR.
  - **Parameters**:
    - `file_paths`: List of input file paths.
    - `output_dir`: Directory to save OCR-processed files.
  - **Returns**: Dictionary with file paths as keys and processing success as values.

- **`get_ocr_status(original_file_path: str) -> Dict[str, Any]`**:
  - Retrieves the OCR status of a file.
  - **Parameters**:
    - `original_file_path`: Path to the original file.
  - **Returns**: Dictionary with OCR status information, including paths and file existence.

- **`cleanup_ocr_file(original_file_path: str) -> bool`**:
  - Removes the OCR file associated with an original file.
  - **Parameters**:
    - `original_file_path`: Path to the original file.
  - **Returns**: `True` if cleanup is successful or the file does not exist, `False` otherwise.

### Notes:
- The class uses a `PDF24OCRClient` to handle the actual OCR processing.
- Logging is used extensively for tracking operations and errors.
- The default OCR configuration supports Arabic and English languages and includes options for deskewing, cleaning, and forcing OCR even if text exists.
- Error handling is implemented to ensure partial files are cleaned up in case of failures.
```
---

## File: controllers/ProcessController.py

Summary:
```markdown
The `ProcessController` class is an enhanced file processing controller that extends a `BaseController`. It is designed to manage file loading, OCR processing, and content chunking. The class supports both text and PDF files, with additional OCR capabilities for PDFs using the `PDF24` OCR tool. Here's a detailed breakdown of its components:

### Class: `ProcessController`
- **Initialization**: 
  - Inherits from `BaseController`.
  - Initializes a logger for logging operations.
  - Initializes an `OCRController` for handling OCR processing.

### Methods:

- **`get_file_extension(file_id: str) -> str`**:
  - Returns the file extension of the given file ID.

- **`get_file_loader(file_id: str) -> Optional[object]`**:
  - Determines the appropriate file loader based on the file extension.
  - Supports text files (`TXT`) using `TextLoader`.
  - Supports PDF files (`PDF`) with optional OCR processing.
  - If OCR is needed and not yet performed, it triggers OCR processing and uses the OCR-processed file.

- **`_get_ocr_file_path(file_id: str) -> str`**:
  - Generates the file path for the OCR-processed version of the file.

- **`get_file_content(file_id: str) -> Optional[list]`**:
  - Loads the file content using the appropriate loader, with OCR support if applicable.

- **`process_file_content(file_content: list, file_id: str, chunk_size: int = 1500, overlap_size: int = 150) -> list`**:
  - Splits the file content into chunks using `RecursiveCharacterTextSplitter`.
  - Configures chunking with custom separators and enhanced metadata.
  - Metadata includes an OCR flag indicating whether OCR processing was applied.

- **`_file_has_ocr(file_id: str) -> bool`**:
  - Checks if the file has been processed with OCR by verifying the existence of the OCR file.

- **`cleanup_ocr_files(file_id: str)`**:
  - Deletes OCR-processed files when the original file is removed.

- **`get_ocr_info(file_id: str) -> dict`**:
  - Provides information about the OCR processing status and file paths for a given file.

### Notes:
- The class uses `langchain_community` document loaders for handling text and PDF files.
- The `RecursiveCharacterTextSplitter` is used for splitting text into manageable chunks with specified chunk size and overlap.
- Logging is used extensively for monitoring OCR processing and file operations.
- The OCR functionality is integrated to enhance PDF processing, especially for scanned documents that require text extraction.
```
---

## File: controllers/ProjectController.py

Summary:
```markdown
The provided code defines a `ProjectController` class, which inherits from a `BaseController` class. This class is designed to manage project directories, specifically handling the creation and retrieval of project paths based on a given project ID.

### Classes and Methods:

- **ProjectController**: 
  - Inherits from `BaseController`.
  - **Constructor (`__init__`)**: Calls the constructor of the base class `BaseController` to initialize any necessary properties or configurations.
  
- **get_project_path(project_id: str)**:
  - **Purpose**: Retrieves the file path for a project using its unique identifier (`project_id`). If the directory for the project does not exist, it creates the directory.
  - **Parameters**: 
    - `project_id` (str): A string representing the unique identifier of the project.
  - **Returns**: 
    - A string representing the path to the project's directory.
  - **Behavior**: 
    - Constructs the full path by joining a base directory (`self.file_dir`) with the `project_id`.
    - Checks if the directory exists; if not, it creates the directory using `os.makedirs()`.

### Notes:
- The code assumes the existence of a `BaseController` class and a `file_dir` attribute within it, which should be defined elsewhere in the codebase.
- The `os` module is used for file path manipulations and directory operations.
- The `UploadFile` and `ResponseSignal` imports are present but not used within the provided code snippet, suggesting potential future extensions or related functionalities.
```
---

## File: controllers/SummarizationController.py

Summary:
```markdown
### Summary of `SummarizationController` Class

The `SummarizationController` class is responsible for managing the summarization process of PDF files. It extends the `BaseController` and utilizes various models and helpers to extract, clean, split, summarize, and merge text from PDF files. The class is designed to handle different content types and languages, and it includes enhanced cleaning and quality checking mechanisms.

#### Initialization
- **`__init__(self, db_client)`**: Initializes the controller with a database client, settings, logger, and instances of `SummaryMerger` and `SummaryQualityChecker`. The summarizer is not initialized here and should be set externally.

#### Main Methods
- **`async create_instance(self)`**: Asynchronously creates and initializes instances of `SummarizationChunkModel` and `AssetModel` using the provided database client.

- **`async summarize_file(self, file_id, language, chunk_size=1500, overlap_size=150, cleaning_level="conservative", content_type="auto", context_aware=True, preserve_formatting=True)`**: Asynchronously summarizes a PDF file. It extracts text, cleans it, detects content type, splits text into chunks, summarizes each chunk, merges summaries, and checks quality. The method handles errors and logs detailed information about the process.

- **`async get_summarization_status(self, file_id)`**: Retrieves the summarization status of a file, indicating whether the process is completed, in progress, or not started.

- **`async get_file_summary_chunks(self, file_id)`**: Retrieves all summary chunks for a specific file.

- **`async get_file_path(self, file_id)`**: Retrieves the file path for a given file ID, checking for an OCR version first.

#### Helper Methods
- **`_detect_document_type(self, cleaned_pages)`**: Detects the document type based on keywords found in the cleaned text.

- **`_manual_merge_summaries(self, summaries, language)`**: Manually merges summaries as a fallback if automatic merging fails.

- **`async _enhanced_quality_check(self, final_summary, chunks, content_type)`**: Performs an enhanced quality check on the final summary, analyzing length, completeness, and quality score.

### Notes
- The class uses various helper functions for text extraction, cleaning, and splitting.
- It supports different cleaning levels (`minimal`, `conservative`, `moderate`) and content types (`auto`, `technical`, `academic`, `legal`, `medical`, `financial`).
- The summarization process is context-aware and can preserve formatting.
- Error handling and logging are integral parts of the process, providing detailed insights into each step.
- The summarizer must be set externally before calling the `summarize_file` method.
```
---

## File: controllers/__init__.py

Summary:
```markdown
This code snippet is an import statement from a Python module or package. It imports several classes from the current package (indicated by the leading dot `.`), which are likely used to manage different aspects of an application. Here's a brief overview of each imported class:

1. **DataController**: This class is likely responsible for handling data-related operations within the application. This could include data retrieval, storage, and manipulation.

2. **BaseController**: This class might serve as a parent or abstract class for other controllers, providing common functionality or interfaces that other controllers extend or implement.

3. **ProjectController**: This class probably manages project-specific operations. This could involve creating, updating, or deleting projects, as well as managing project metadata.

4. **ProcessController**: This class is likely responsible for handling processes within the application. This might include starting, stopping, or monitoring various processes that the application runs.

5. **NLPController**: This class is probably focused on natural language processing (NLP) tasks. It might include methods for text analysis, language modeling, or other NLP-related functionalities.

**Notes for Future Reference:**
- The use of relative imports (indicated by the `.`) suggests that these classes are part of the same package or module hierarchy.
- Understanding the specific methods and attributes of each class would require examining their respective definitions in the source files (`DataController.py`, `BaseController.py`, etc.).
- These controllers might interact with each other, so understanding their relationships and dependencies could be crucial for maintaining or extending the application.
```
---

## File: fix_app_state.sh

Summary:
```markdown
This Bash script is designed to update specific paths in a set of Python route files. The script modifies references to application state objects in the code by replacing them with a new path format. This is useful for ensuring consistency and correctness in accessing application state within a web application.

### Key Components:

- **File List (`files`)**: An array containing the paths to the Python files that need to be updated. These files are located in the `routes` directory.

- **Replacements (`replacements`)**: An associative array (dictionary) where each key-value pair represents a string to be replaced (`key`) and its replacement (`value`). The replacements involve changing the path from `request.app.<object>` to `request.app.state.<object>` and similarly for `app_request`.

- **Processing Loop**: The script iterates over each file in the `files` array:
  - Checks if the file exists.
  - If the file exists, it applies all replacements using the `sed` command, which performs in-place substitution of the old paths with the new paths.
  - If the file does not exist, it outputs a warning message.

- **Output Messages**: The script provides feedback on the processing status of each file, indicating whether it was successfully updated or if it was not found.

### Usage Notes:

- **Execution Environment**: This script should be run in a Unix-like environment where Bash and `sed` are available.
- **File Permissions**: Ensure that the script has execution permissions and that the user has write access to the files being modified.
- **Backup**: It's advisable to back up the original files before running the script, as changes are made in-place and cannot be undone automatically.
- **Localization**: The script includes comments in Arabic, indicating it might be intended for users familiar with the language.
```
---

## File: helpers/__init__.py

Summary:
```markdown
It seems there is no code provided. Please provide the code you would like me to document, and I will be happy to assist!
```
---

## File: helpers/config.py

Summary:
```markdown
This code defines a `Settings` class using the Pydantic library to manage application configuration settings. These settings are typically loaded from a `.env` file. The class includes various configurations for application behavior, AI model settings, summarization, OCR (Optical Character Recognition), and other functionalities. Here's a breakdown of the key components:

### `Settings` Class
- **Basic Application Settings**: Includes `APP_NAME`, `APP_VERSION`, and file handling configurations like `FILE_ALLOWED_TYPE`, `FILE_MAX_SIZE`, and `FILE_DEFAULT_CHUNK_SIZE`.
- **AI Backend Settings**: Configures AI-related settings such as `GENERATION_BACKEND`, `EMBEDDING_BACKEND`, and model identifiers.
- **API Keys**: Stores optional API keys for different providers like OpenAI, Cohere, and Gemini.
- **Summarization Settings**: Configures various aspects of text summarization, including model settings, language preferences, chunk processing, and quality checks.
- **OCR Settings**: Manages OCR service configurations, processing options, file management, quality settings, and error handling.
- **Language Settings**: Defines primary and default languages for the application.

### Methods
- `get_summarization_model_config()`: Returns a dictionary with summarization model configuration settings.
- `get_quality_check_config()`: Returns a dictionary with quality check configuration settings.
- `get_processing_config()`: Returns a dictionary with processing configuration settings.

### Configuration
- **Pydantic Configuration**: Uses `SettingsConfigDict` to specify the `.env` file and manage extra environment variables.

### `get_settings()` Function
- A utility function that returns an instance of the `Settings` class, effectively loading the configuration from the environment.

This setup allows for flexible and dynamic configuration management, making it easy to adjust application settings without modifying the code directly. The use of Pydantic ensures that the settings are validated and typed correctly.
```
---

## File: helpers/pdf_ocr_processor.py

Summary:
```markdown
The provided code defines a `PDF24OCRClient` class, which is a client for performing OCR (Optical Character Recognition) operations on PDF files using the PDF24 online service. The class handles the entire process of uploading a PDF file, performing OCR, and downloading the processed result.

### Class: `PDF24OCRClient`

#### Constants:
- `BASE_URL`: Base URL for the PDF24 service.
- `DEFAULT_TIMEOUT`: Default timeout for requests.
- `MAX_RETRIES`: Maximum number of retries for failed requests.
- `BACKOFF_FACTOR`: Backoff factor for retries.
- `STATUS_FORCELIST`: HTTP status codes that trigger a retry.

#### Methods:

- **`__init__(session: Optional[requests.Session] = None, timeout: int = DEFAULT_TIMEOUT)`**: Initializes the client with an optional custom session and timeout.

- **`_create_session() -> requests.Session`**: Creates a requests session with retry strategy and connection pooling.

- **`_get_worker_servers() -> None`**: Retrieves available worker servers from PDF24.

- **`_select_random_server() -> str`**: Selects a random worker server from the available servers.

- **`_start_session() -> str`**: Starts a session with the selected worker server and returns the session ID cookie.

- **`_initialize_tracking() -> None`**: Initializes tracking services for analytics (optional).

- **`upload_file(file_path: str) -> Dict[str, Any]`**: Uploads a PDF file to the server and returns the upload response.

- **`start_ocr(files_data: List[Dict[str, Any]], config: Optional[Dict[str, Any]] = None) -> str`**: Starts OCR processing on uploaded files and returns the job ID.

- **`wait_for_completion(job_id: str, server: str, max_wait_time: int = 300, check_interval: int = 2) -> None`**: Waits for the OCR job to complete.

- **`download_result(job_id: str, server: str, output_path: str) -> None`**: Downloads the processed file.

- **`process_pdf(input_path: str, output_path: str, config: Optional[Dict[str, Any]] = None) -> None`**: Completes the OCR process by uploading, processing, and downloading the file.

### Example Usage
The `main()` function demonstrates how to use the `PDF24OCRClient` class to process a PDF file. It initializes the client, optionally configures OCR settings, and processes the PDF file from `input_path` to `output_path`.

### Notes:
- The code includes error handling for various exceptions, such as file not found, request failures, and timeout errors.
- The client uses a retry strategy to handle transient HTTP errors.
- Tracking initialization is optional and does not affect the core functionality of the OCR process.
```
---

## File: helpers/pdf_parser.py

Summary:
```markdown
This code provides functionality to extract text and metadata from PDF files using two different libraries: `pdfplumber` and `PyMuPDF`. It includes error handling and logging to facilitate debugging and ensure robustness.

### Functions:

1. **`extract_text_by_page(file_path: str, method: str = 'pdfplumber') -> Dict[int, str]`**:
   - Extracts text from a PDF file while maintaining page order.
   - **Parameters**:
     - `file_path`: The path to the PDF file.
     - `method`: The extraction method to use, either 'pdfplumber' or 'pymupdf'.
   - **Returns**: A dictionary mapping page numbers to their extracted text.

2. **`_extract_with_pdfplumber(file_path: str) -> Dict[int, str]`**:
   - Uses `pdfplumber` to extract text from each page of the PDF.
   - Cleans the extracted text and adds references to images and tables if detected.

3. **`_extract_with_pymupdf(file_path: str) -> Dict[int, str]`**:
   - Uses `PyMuPDF` to extract text from each page of the PDF.
   - Cleans the extracted text and adds references to images and tables if detected.

4. **`_clean_extracted_text(text: str) -> str`**:
   - Cleans the extracted text by removing unwanted characters, fixing line breaks, and preserving paragraphs.

5. **`_add_image_table_references(page, text: str, page_num: int) -> str`**:
   - Adds references to images and tables in the text for `pdfplumber`.

6. **`_add_visual_elements_references(page, text: str, page_num: int) -> str`**:
   - Adds references to images and tables in the text for `PyMuPDF`.

7. **`extract_metadata(file_path: str) -> Dict[str, Any]`**:
   - Extracts metadata from the PDF file using `pdfplumber`.
   - **Returns**: A dictionary containing metadata such as total pages, author, title, subject, creator, creation date, modification date, and language.

### Notes:
- The code uses logging to capture errors and warnings, which are logged using the `uvicorn.error` logger.
- The text extraction methods include fallback mechanisms and error handling to ensure that text is extracted even if one method fails.
- The code includes Arabic comments, indicating that it might be intended for use in an Arabic-speaking context or by developers familiar with Arabic.
```
---

## File: helpers/summarization_text_splitter.py

Summary:
```markdown
This code provides functionality to split a given text into smaller chunks suitable for summarization, while maintaining context and ensuring the completeness of sentences. It uses the `RecursiveCharacterTextSplitter` from the `langchain` library to handle the text splitting process. The code includes several helper functions to prepare the text, process each chunk, and merge small chunks. Here's a detailed breakdown of the functions:

1. **`split_text_for_summarization`**: 
   - Splits the input text into chunks based on specified chunk size and overlap size.
   - Parameters:
     - `text`: The text to be split.
     - `chunk_size`: The maximum size of each chunk.
     - `overlap_size`: The overlap size between consecutive chunks.
     - `page_number`: Optional page number for reference.
   - Returns a list of dictionaries, each containing chunk information such as text, chunk ID, word count, and additional metadata.

2. **`_prepare_text_for_splitting`**:
   - Prepares the text by formatting headers and lists, and removing extra spaces.
   - Uses regular expressions to identify and format specific patterns.

3. **`_process_chunk`**:
   - Processes each chunk to ensure sentence completeness and gather metadata.
   - Checks for headers, lists, and numbers within the chunk.
   - Returns a dictionary with processed chunk data or `None` if the chunk is too small.

4. **`_ensure_sentence_completeness`**:
   - Ensures that each chunk starts and ends with a complete sentence.
   - Adjusts the chunk by searching for sentence boundaries using punctuation.

5. **`_check_for_headers`, `_check_for_lists`, `_check_for_numbers`**:
   - These functions check for the presence of headers, lists, and numbers within a chunk using regular expressions.

6. **`_merge_small_chunks`**:
   - Merges chunks that are smaller than a specified minimum size to form larger, more meaningful chunks.

7. **`create_contextual_chunks`**:
   - Extends the basic chunking by adding context from adjacent chunks to each chunk.
   - Useful for providing additional context that can improve summarization quality.

The code is structured to handle text processing robustly, with logging for error tracking and debugging. It is designed to be flexible, allowing customization of chunk sizes and overlap for different summarization needs.
```
---

## File: helpers/text_cleaner.py

Summary:
```markdown
This code provides a set of functions for cleaning and processing text, particularly text extracted from PDFs. The primary goal is to clean the text while preserving important information. Here's a breakdown of the main components:

### Functions

1. **`clean_text(text: str, aggressive: bool = False) -> str`**
   - Cleans the given text with an option for aggressive cleaning.
   - Parameters:
     - `text`: The text to be cleaned.
     - `aggressive`: If `True`, applies more comprehensive cleaning.
   - Returns the cleaned text.

2. **`_safe_basic_cleanup(text: str) -> str`**
   - Performs basic and safe cleaning by removing dangerous non-printable characters, reducing excessive spaces, and trimming the text.

3. **`_advanced_cleaning(text: str) -> str`**
   - Applies advanced cleaning techniques, such as fixing broken words between lines, correcting punctuation issues, and managing spaces around punctuation.

4. **`_preserve_important_elements(text: str) -> str`**
   - Protects important elements like numbers, dates, and monetary values from being altered during cleaning.

5. **`_restore_protected_elements(text: str) -> str`**
   - Restores the protected elements back to their original form after cleaning.

6. **`conservative_clean(text: str) -> str`**
   - Applies a very conservative cleaning approach, ensuring minimal loss of important information.

7. **`moderate_clean(text: str) -> str`**
   - Balances between cleaning and preserving content, using both basic and limited advanced cleaning.

8. **`minimal_clean(text: str) -> str`**
   - Performs minimal cleaning by removing only dangerous characters and trimming spaces.

9. **`extract_special_elements(text: str) -> Dict[str, List[str]]`**
   - Extracts special elements like headers, lists, numbers, dates, URLs, and emails from the text without deleting them.

10. **`validate_cleaned_text(original: str, cleaned: str) -> Dict[str, any]`**
    - Validates that the cleaning process did not remove important information, providing statistics and warnings if necessary.

11. **`clean_for_summarization(text: str) -> str`**
    - Cleans text specifically for summarization, ensuring important information is preserved while improving paragraph structure.

### Notes

- The code uses regular expressions extensively for pattern matching and text manipulation.
- Logging is used for debugging and error reporting.
- The functions are designed to handle Arabic text, as indicated by the use of Arabic characters in regex patterns and comments.
- The `clean_text` function is redefined to default to a conservative cleaning approach, emphasizing the preservation of important content.
```
---

## File: main.py

Summary:
```markdown
This code sets up a FastAPI application with various configurations and middleware, including CORS and session management. It integrates multiple routers for handling different routes and functionalities, such as base, data, NLP, integrated, asset settings, and summarization. The application also connects to MongoDB and initializes several models and controllers for handling assets, chunks, and summarization tasks.

### Key Components:

- **FastAPI Application**: Created using `FastAPI(lifespan=lifespan)`, which includes a custom lifespan manager for startup and shutdown processes.
- **Middleware**:
  - `CORSMiddleware`: Allows cross-origin requests from any origin.
  - `SessionMiddleware`: Manages sessions with a secret key.
- **Database Connection**: Connects to MongoDB using `AsyncIOMotorClient` and initializes database clients for various models.
- **Provider Factories**: Initializes provider factories for LLM, VectorDB, and Summarization services.
- **Controllers**:
  - `NLPController`: Manages NLP-related tasks using various clients and parsers.
  - `SummarizationController`: Handles summarization tasks and integrates with the summarization provider.
- **Routers**: Includes routers for different functionalities, ensuring modularity and separation of concerns.
- **Exception Handling**: A global exception handler is defined to catch and log errors, returning a generic error message to the client.

### Functions and Classes:

- **`lifespan(app: FastAPI)`**: An async context manager that handles the startup and shutdown processes of the application. It initializes settings, database connections, models, and various service providers.
- **`global_exception_handler(request, exc)`**: Catches all exceptions, logs them, and returns a standardized error response.

### Notes:

- The application uses a variety of external libraries, including `motor` for MongoDB, and custom modules for routes, models, and controllers.
- The code includes Arabic comments, indicating that it may be intended for a multilingual audience or development team.
- The configuration and initialization processes are highly modular, allowing for easy adjustments and scalability.
```
---

## File: mk.py

Summary:
```markdown
This code snippet is a Python script that creates a text file containing information about Retrieval-Augmented Generation (RAG) technology, written in Arabic. The script performs the following tasks:

1. **Text Definition (`rag_text`)**: The script starts by defining a multi-line string `rag_text` that contains a detailed description of RAG technology, its mechanism, applications, challenges, and future prospects. The text is structured into sections with headings and subheadings.

2. **File Creation**: The script opens a new text file named `rag_info.txt` in write mode with UTF-8 encoding to support Arabic characters. It writes the content of `rag_text` into this file.

3. **Confirmation Message**: After successfully writing to the file, the script prints a confirmation message in Arabic indicating that the file has been created successfully.

### Key Components:

- **`rag_text`**: A string variable holding the content to be written to the file. It provides an overview of RAG technology, including its definition, working mechanism, applications, challenges, and future outlook.

- **File Handling**: The `open` function is used with the `"w"` mode to create and write to a file, and `encoding="utf-8"` ensures proper handling of Arabic text.

- **Output**: A print statement confirms the successful creation of the file.

This script is useful for generating documentation or educational material about RAG technology in Arabic, and it demonstrates basic file handling in Python.
```
---

## File: models/AssetModel.py

Summary:
```markdown
The `AssetModel` class is a data model for managing asset-related operations in a database. It extends the `BaseDataModel` class and is designed to interact with a MongoDB collection specifically for assets. The class provides methods to initialize the collection, create assets, and retrieve assets by various criteria.

### Class: `AssetModel`

- **Constructor (`__init__`)**: 
  - Initializes the `AssetModel` with a database client and sets the collection to the assets collection defined in `DataBaseEnum.COLLECTION_ASSETS_NAME`.

- **Method: `create_instance`**:
  - A class method that asynchronously creates an instance of `AssetModel` and initializes the collection.
  - Parameters:
    - `db_client`: The database client object used to interact with the database.

- **Method: `init_collection`**:
  - Asynchronously initializes the asset collection by checking if it exists and creating necessary indexes if it doesn't.
  - Utilizes the `Asset.get_indexes()` method to retrieve index configurations.

- **Method: `create_asset`**:
  - Asynchronously inserts a new asset into the collection.
  - Parameters:
    - `asset`: An instance of the `Asset` class containing asset data.
  - Returns the asset with its `id` set to the newly inserted document's ID.

- **Method: `get_all_assets`**:
  - Asynchronously retrieves all assets, optionally filtered by asset type, with pagination support.
  - Parameters:
    - `asset_type`: (Optional) A string to filter assets by type.
    - `page`: The page number for pagination.
    - `page_size`: The number of assets per page.
  - Returns a tuple containing a list of `Asset` instances and the total number of pages.

- **Method: `get_asset_by_name`**:
  - Asynchronously retrieves an asset by its name.
  - Parameters:
    - `asset_name`: The name of the asset to retrieve.
  - Returns an `Asset` instance if found, otherwise `None`.

- **Method: `get_asset_by_id`**:
  - Asynchronously retrieves an asset by its ID.
  - Parameters:
    - `asset_id`: The ID of the asset to retrieve, which can be a string or an `ObjectId`.
  - Returns an `Asset` instance if found, otherwise `None`.

### Notes:
- The class relies on the `bson.ObjectId` for handling MongoDB document IDs.
- It uses asynchronous operations to interact with the database, which is suitable for non-blocking applications.
- The `Asset` class is assumed to be a data model that provides methods like `model_dump` and `get_indexes`.
- The `DataBaseEnum` is used to manage collection names, ensuring consistency across the application.
```
---

## File: models/AssetSettingsModel.py

Summary:
```markdown
The `AssetSettingsModel` class is designed to manage asset settings within a database. It extends the `BaseDataModel` and provides methods to initialize, retrieve, update, and manage asset settings. The class interacts with a MongoDB collection dedicated to asset settings and includes methods to handle default settings and available models for different providers.

### Class: `AssetSettingsModel`

- **Constructor (`__init__`)**: Initializes the class with a database client, sets up the collection for asset settings, and loads application configuration settings.

- **Method: `create_instance`**: A class method that asynchronously creates and returns an instance of `AssetSettingsModel`, initializing the collection if necessary.

- **Method: `init_collection`**: Asynchronously initializes the asset settings collection in the database if it does not exist, creating necessary indexes as defined in `AssetSettings`.

- **Method: `get_asset_settings`**: Asynchronously retrieves settings for a specific asset by `asset_id`. If no settings are found, it returns default settings.

- **Method: `update_asset_settings`**: Asynchronously updates the settings for a specific asset using `asset_id` and `user_id`. It also updates the modification timestamp and supports upsert operations.

- **Method: `get_default_settings`**: Returns default settings for an asset using application configuration values.

- **Method: `get_available_models`**: Retrieves a list of available models for a given provider, returning model details such as ID, name, and maximum tokens.

### Notes

- The class relies on external modules and configurations, such as `AssetSettings`, `DataBaseEnum`, and `get_settings`, which are assumed to be defined elsewhere in the codebase.
- The database operations are asynchronous, indicating the use of an asynchronous MongoDB client.
- The class supports multiple providers for model retrieval, including "OPENAI", "GEMINI", "COHERE", and "DEEPSEEK", each with predefined models and token limits.
```
---

## File: models/BaseDataModel.py

Summary:
```markdown
The provided code defines a class `BaseDataModel` that serves as a foundational class for data models interacting with a database. It includes the following components:

- **`BaseDataModel` Class**: 
  - **Purpose**: To initialize a base data model with a database client and application settings.
  - **Constructor (`__init__`)**: 
    - **Parameters**:
      - `db_client`: An object representing the database client, which is used for database interactions.
    - **Attributes**:
      - `self.db_client`: Stores the provided database client for use in database operations.
      - `self.settings`: Stores application settings retrieved by calling `get_settings()`, which is assumed to be a function that returns an instance of `Settings`.

- **Dependencies**:
  - `get_settings`: A function imported from `helpers.config`, presumably returning configuration settings for the application.
  - `Settings`: A class or type hint imported from `helpers.config`, representing the configuration settings structure.

This class is designed to be extended by other data models that require database interaction and access to application settings.
```
---

## File: models/ChunkModel.py

Summary:
```markdown
The `ChunkModel` class is a data model for managing "chunks" in a MongoDB collection. It extends the `BaseDataModel` and provides asynchronous methods to interact with the database. The class is designed to handle operations related to chunks, which are presumably parts of a larger data structure, such as a file or document.

### Key Components:

- **Initialization:**
  - `__init__(self, db_client: object)`: Initializes the `ChunkModel` with a database client and sets the collection to the one specified by `DataBaseEnum.COLLECTION_CHUNK_NAME`.
  - `create_instance(cls, db_client: object)`: Class method to create an instance of `ChunkModel` and initialize the collection.

- **Collection Management:**
  - `init_collection(self)`: Ensures the collection is initialized with the necessary indexes, including a new index on `chunk_order` to optimize search operations.

- **CRUD Operations:**
  - `create_chunk(self, chunk: DataChunk)`: Inserts a new chunk into the collection and returns it with its assigned ID.
  - `get_chunk(self, chunk_id: str)`: Retrieves a chunk by its ID.
  - `insert_many_chunks(self, chunks: list, batch_size: int = 400)`: Inserts multiple chunks in batches to optimize performance.
  - `delete_chunks_by_asset_id(self, asset_id: ObjectId)`: Deletes all chunks associated with a specific asset ID.

- **Chunk Retrieval and Search:**
  - `get_asset_chunks(self, asset_id: ObjectId, page_no: int = 1, page_size: int = 50)`: Retrieves chunks for a specific asset with pagination.
  - `get_adjacent_chunks(self, asset_id: ObjectId, current_order: int, context_size: int = 1)`: Retrieves chunks adjacent to a specified chunk, excluding the chunk itself.
  - `get_chunk_with_context(self, chunk_id: str, context_size: int = 1)`: Retrieves a chunk along with its adjacent chunks.
  - `get_chunk_by_order(self, asset_id: ObjectId, order: int)`: Retrieves a chunk by its order within an asset.
  - `search_chunks(self, asset_id: Optional[ObjectId] = None, query_text: str = None, limit: int = 10)`: Searches for chunks using a simple text query, optionally filtered by asset ID.

### Notes:
- The class uses `pymongo` for database operations and `bson.ObjectId` for handling MongoDB object IDs.
- The `DataChunk` class is assumed to define the schema for chunks and provides methods like `model_dump`.
- The class supports asynchronous operations, making it suitable for use in applications that require non-blocking database interactions.
- The code includes comments in Arabic, which provide additional context for some methods.
```
---

## File: models/ProjectModel.py

Summary:
```markdown
The provided code defines a `ProjectModel` class, which is a data access layer for managing project data in a database. It extends a base class `BaseDataModel` and uses an asynchronous database client to interact with a collection of projects. The class is designed to work with a MongoDB-like database, as suggested by the use of methods like `insert_one` and `find_one`.

### Class: `ProjectModel`
- **Constructor (`__init__`)**: Initializes the `ProjectModel` with a database client and sets the collection to the one specified by `DataBaseEnum.COLLECTION_PROJECT_NAME`.
  - `db_client`: An object representing the database client.

- **Class Method: `create_instance`**: Asynchronously creates and initializes an instance of `ProjectModel`.
  - `db_client`: An object representing the database client.
  - Returns an initialized `ProjectModel` instance.

- **Method: `init_collection`**: Asynchronously initializes the collection by checking if it exists and creating necessary indexes if it doesn't.
  - Uses `Project.get_indexes()` to retrieve index configurations.

- **Method: `create_project`**: Asynchronously inserts a new project into the collection and returns the project with its database-assigned ID.
  - `project`: An instance of the `Project` class to be inserted.

- **Method: `get_project_or_create_one`**: Asynchronously retrieves a project by its ID or creates a new one if it doesn't exist.
  - `project_id`: A string representing the unique identifier of the project.

- **Method: `get_all_projects`**: Asynchronously retrieves all projects with pagination support.
  - `page`: An integer representing the current page number (default is 1).
  - `page_size`: An integer representing the number of projects per page (default is 10).
  - Returns a tuple containing a list of `Project` instances and the total number of pages.

### Notes
- The code assumes the existence of a `Project` class with a method `get_indexes()` and a method `model_dump()` for serialization.
- The `DataBaseEnum` is used to define database-related constants, specifically the collection name.
- The `BaseDataModel` class is a superclass that likely provides common database functionalities.
- The use of asynchronous methods (`async def`) and `await` indicates that the operations are non-blocking and designed to work with an asynchronous event loop, typical in frameworks like `asyncio` or `FastAPI`.
```
---

## File: models/SummarizationChunkModel.py

Summary:
```markdown
The `SummarizationChunkModel` class is designed to manage summarization chunks in a database. It inherits from `BaseDataModel` and interacts with a MongoDB collection to perform various operations related to summarization chunks. Below is a detailed breakdown of its components and functionality:

### Class: `SummarizationChunkModel`

- **Purpose**: Manages summarization chunks in a database, providing methods to create, retrieve, update, delete, and analyze these chunks.

#### Initialization
- **`__init__(self, db_client)`**: Initializes the model with a database client and sets up the collection for summarization chunks using a specific collection name from `DataBaseEnum`.

#### Class Methods
- **`create_instance(cls, db_client)`**: Asynchronously creates and initializes an instance of the model, ensuring the collection is set up.

#### Instance Methods
- **`init_collection(self)`**: Initializes the summarization chunks collection by creating it if it doesn't exist and setting up necessary indexes.

- **`create_summary_chunk(self, chunk_data: Dict[str, Any]) -> ObjectId`**: Inserts a new summarization chunk into the collection and returns its `ObjectId`.

- **`get_summary_chunks(self, file_id: str, page_number: Optional[int] = None) -> List[Dict[str, Any]]`**: Retrieves summarization chunks for a specific file, optionally filtered by page number.

- **`update_summary_chunk(self, chunk_id: ObjectId, update_data: Dict[str, Any]) -> bool`**: Updates a summarization chunk with new data and returns a boolean indicating success.

- **`delete_summary_chunks(self, file_id: str) -> int`**: Deletes all summarization chunks for a specific file and returns the count of deleted chunks.

- **`save_final_summary(self, file_id: str, summary_data: Dict[str, Any]) -> ObjectId`**: Saves a final summary for a file, replacing any existing final summary, and returns its `ObjectId`.

- **`get_final_summary(self, file_id: str) -> Optional[Dict[str, Any]]`**: Retrieves the final summary for a file, if it exists.

- **`get_summarization_progress(self, file_id: str) -> Dict[str, Any]`**: Provides progress information on the summarization process for a file, including processed chunks and completion status.

- **`search_summary_chunks(self, file_id: str, search_query: str) -> List[Dict[str, Any]]`**: Searches for summarization chunks containing the search query in either the original or summary text.

- **`get_chunk_statistics(self, file_id: str) -> Dict[str, Any]`**: Computes and returns statistics about the summarization chunks for a file, such as total chunks, character counts, compression ratios, and pages covered.

### Notes
- The class uses asynchronous database operations, suitable for environments where non-blocking I/O is required.
- Logging is used extensively to track operations and errors, aiding in debugging and monitoring.
- The class assumes the presence of a MongoDB client and a specific database schema, including fields like `file_id`, `chunk_order`, `page_number`, `chunk_size`, and `summary_size`.
- The use of `DataBaseEnum` suggests a structured way to manage database-related constants, improving maintainability.
```
---

## File: models/__init__.py

Summary:
```markdown
This code snippet imports two enumerations from different modules within the same package. 

1. **`ResponseSignal`**: This enumeration is imported from the `ResponseEnums` module located in the `enums` package. While the specific members of this enumeration are not detailed here, it is likely used to represent various response signals or statuses within the application.

2. **`ProcessingEnum`**: This enumeration is imported from the `ProcessingEnum` module, also within the `enums` package. Similar to `ResponseSignal`, this enumeration likely defines different processing states or types relevant to the application's logic.

### Notes for Other ChatBots:
- Ensure that the `enums` package is correctly structured and accessible within the project for these imports to work.
- The specific members and their purposes within each enumeration should be documented in their respective modules (`ResponseEnums` and `ProcessingEnum`) for clarity.
- These imports suggest a design pattern where enumerations are used to manage and represent fixed sets of constants, improving code readability and maintainability.
```
---

## File: models/db_schemes/AssetSettings.py

Summary:
```markdown
The provided code defines a `Pydantic` model class `AssetSettings` for managing settings related to language generation for assets. This class is designed to handle various configurations for each asset, allowing customization of generation settings and language preferences.

### Class: `AssetSettings`

- **Purpose**: Manages settings for language generation associated with a specific asset.
- **Attributes**:
  - `id` (Optional[ObjectId]): MongoDB ObjectId, aliased as `_id`.
  - `asset_id` (str): Identifier for the asset.
  - `user_id` (str, optional): Identifier for the user who created the settings.
  - `generation_backend` (str): Specifies the language generation provider (default is "OPENAI"). Options include "OPENAI", "GEMINI", "COHERE".
  - `generation_model_id` (str, optional): Identifier for the language generation model.
  - `generation_max_tokens` (int): Maximum number of tokens for generation (default is 500).
  - `generation_temperature` (float): Temperature setting for generation, affecting randomness (default is 0.1).
  - `primary_language` (str): Preferred language for generation (default is "en").
  - `created_at` (datetime): Timestamp for when the settings were created, defaults to the current UTC time.
  - `updated_at` (datetime): Timestamp for the last update, defaults to the current UTC time.

- **Configuration**:
  - `arbitrary_types_allowed`: Allows arbitrary types like `ObjectId`.
  - `json_encoders`: Custom encoder for `ObjectId` to convert it to a string.

- **Methods**:
  - `get_indexes()`: Class method that returns a list of index specifications for database indexing. It specifies a unique index on the `asset_id` field to ensure each asset has distinct settings.

### Notes:
- The class uses `Pydantic` for data validation and management, ensuring that the data conforms to the specified types and constraints.
- The `get_indexes` method is useful for database operations, particularly when using MongoDB, to ensure efficient querying and uniqueness of asset settings.
- The use of `Field` with `default_factory` for `created_at` and `updated_at` ensures that timestamps are automatically set to the current time when an instance is created.
```
---

## File: models/db_schemes/Project.py

Summary:
```markdown
This code defines a `Project` class using the Pydantic library, which is a data validation and settings management library in Python. The class is designed to model a project with specific attributes and validation rules.

### Class: `Project`
- **Attributes:**
  - `id`: An optional `ObjectId` field, which is used to represent the unique identifier for the project in a MongoDB collection. It is aliased as `_id`.
  - `project_id`: A required string field with a minimum length of 1. It represents the unique identifier for the project within the application.

- **Methods:**
  - `validate_project_id(cls, value)`: A validator method for the `project_id` field. It ensures that the `project_id` is alphanumeric. If the value is not alphanumeric, it raises a `ValueError`.
  - `get_indexes(cls)`: A class method that returns a list of index specifications for MongoDB. It specifies an index on the `project_id` field, ensuring it is unique.

- **Config:**
  - `arbitrary_types_allowed`: Set to `True` to allow the use of arbitrary types such as `ObjectId` in the Pydantic model.

### Notes:
- The use of `ObjectId` suggests that this model is intended to be used with a MongoDB database.
- The `get_indexes` method is useful for ensuring that the `project_id` field is indexed in the database, which can improve query performance and enforce uniqueness.
- The use of Pydantic's `Field` and `validator` helps enforce data integrity and validation rules, ensuring that any instance of `Project` adheres to the specified constraints.
```
---

## File: models/db_schemes/__init__.py

Summary:
```markdown
This code snippet is a set of import statements from a Python package, indicating that it is part of a larger module or application. Here's a breakdown of the components being imported:

1. **Project**: This is a class imported from the `Project` module within the same package. The `Project` class likely represents a project entity, encapsulating attributes and methods relevant to managing or interacting with projects.

2. **DataChunk**: This is a class imported from the `data_chunk` module. The `DataChunk` class probably deals with segments or portions of data, possibly for processing or analysis purposes.

3. **RetrieveDocument**: This is another import from the `data_chunk` module, suggesting it is a function or class responsible for retrieving documents. This could involve fetching documents from a database, file system, or other storage mediums.

4. **Asset**: This is a class imported from the `asset` module. The `Asset` class likely represents a resource or item of value within the application, which could be anything from digital files to physical resources.

### Notes:
- The use of relative imports (indicated by the leading dot) suggests that this code is part of a package or sub-package structure.
- Understanding the specific functionality of each class or function would require examining their respective modules (`Project`, `data_chunk`, and `asset`).
- The imports suggest a modular design, where different aspects of the application (projects, data handling, and assets) are separated into distinct modules for better organization and maintainability.
```
---

## File: models/db_schemes/asset.py

Summary:
```markdown
The provided code defines a Pydantic model named `Asset` for managing asset data, likely intended for use with a MongoDB database. Here's a breakdown of the code:

### Class: `Asset`
- **Purpose**: Represents an asset with various attributes, and provides a method to define database indexes.
- **Attributes**:
  - `id` (Optional[ObjectId]): The unique identifier for the asset, mapped to the MongoDB `_id` field.
  - `asset_type` (str): The type of the asset, must be a non-empty string.
  - `asset_name` (str): The name of the asset, must be a non-empty string.
  - `asset_size` (int): The size of the asset, must be a non-negative integer. Defaults to `None`.
  - `asset_config` (dict): A dictionary to store configuration details of the asset. Defaults to `None`.
  - `asset_pushed_at` (datetime): Timestamp indicating when the asset was pushed, defaults to the current UTC time at the moment of object creation.

- **Config**:
  - `arbitrary_types_allowed`: Set to `True` to allow the use of `ObjectId` from the `bson` library, which is not a standard Python type.

- **Method**:
  - `get_indexes(cls)`: A class method that returns a list of index specifications for MongoDB. It defines an index on the `asset_name` field, ensuring uniqueness to prevent duplicate asset names.

### Notes
- The use of `Field` from Pydantic allows for detailed validation and default value management.
- The `get_indexes` method is useful for setting up database constraints and optimizing queries based on asset names.
- The `datetime.utcnow` function is used to set the default value for `asset_pushed_at`, ensuring that the timestamp is recorded in UTC.
```
---

## File: models/db_schemes/data_chunk.py

Summary:
```markdown
This code defines two classes, `DataChunk` and `RetrieveDocument`, using the Pydantic library for data validation and modeling. It also uses the `bson` library for handling MongoDB's `ObjectId`.

### Classes and Their Attributes:

1. **DataChunk**:
   - Inherits from `BaseModel` provided by Pydantic.
   - **Attributes**:
     - `id`: An optional `ObjectId` representing the unique identifier for the data chunk, aliased as `_id`.
     - `chunk_text`: A required string field with a minimum length of 1, representing the text content of the chunk.
     - `chunk_metadata`: A dictionary to store additional metadata about the chunk.
     - `chunk_order`: An integer greater than 0, indicating the order of the chunk.
     - `chunk_asset_id`: An `ObjectId` linking the chunk to a specific asset.
   - **Config**:
     - `arbitrary_types_allowed`: Set to `True` to allow non-standard types like `ObjectId`.
   - **Methods**:
     - `get_indexes`: A class method that returns a list of index specifications for MongoDB, specifically creating an index on `chunk_asset_id`.

2. **RetrieveDocument**:
   - Inherits from `BaseModel`.
   - **Attributes**:
     - `text`: A string representing the document's text.
     - `score`: A float representing the document's score, possibly for ranking or relevance.

### Notes:
- The `DataChunk` class is designed to facilitate the storage and retrieval of text data chunks, likely for a document database like MongoDB, given the use of `ObjectId`.
- The `get_indexes` method in `DataChunk` is intended for database indexing, which can improve query performance.
- The `RetrieveDocument` class is a simple model likely used for returning search results or similar operations, with a focus on text and scoring.
```
---

## File: models/enums/AssetTypeEnum.py

Summary:
```markdown
The provided code defines an enumeration class `AssetTypeEnum` using Python's `enum` module. This class is used to represent different types of assets, with the current implementation including a single asset type:

- `FILE`: Represents a file asset type, with the string value "file".

### Key Components:

- **Enum Class (`AssetTypeEnum`)**: This is a subclass of Python's `Enum` class, which is used to define a set of named constants. In this case, it is used to categorize asset types.

- **Member (`FILE`)**: A member of the `AssetTypeEnum` that represents a file asset. It is assigned the string value `"file"`.

### Usage:

This enum can be used in applications where asset types need to be specified or checked, providing a clear and type-safe way to handle different asset types. Additional asset types can be added to this enum as needed by defining more members in a similar manner.

### Notes:

- Enums are useful for creating a set of named values that are constant and can be compared by identity.
- This enum currently only includes one type, but it can be expanded to include more asset types as required by the application.
```
---

## File: models/enums/DataBaseEnum.py

Summary:
```markdown
The provided code defines an enumeration class `DataBaseEnum` using Python's `enum` module. This class is designed to represent various database collection names, which can be used throughout a codebase to ensure consistency and avoid hardcoding string literals for collection names.

### Class: `DataBaseEnum`
- **Purpose**: To define a set of constants representing the names of collections in a database.
- **Base Class**: Inherits from `Enum`, which allows for creating enumerations, a set of symbolic names bound to unique, constant values.

### Enum Members:
- `COLLECTION_PROJECT_NAME`: Represents the collection name for projects, with the value `"projects"`.
- `COLLECTION_CHUNK_NAME`: Represents the collection name for chunks, with the value `"chunks"`.
- `COLLECTION_ASSETS_NAME`: Represents the collection name for assets, with the value `"assets"`.
- `COLLECTION_ASSET_SETTINGS_NAME`: Represents the collection name for asset settings, with the value `"asset_settings"`. The comment in Arabic suggests this is an addition for file settings.
- `COLLECTION_SUMMARIZATION_CHUNKS_NAME`: Represents the collection name for summarization chunks, with the value `"summarization_chunks"`.

### Notes:
- Using an `Enum` for database collection names helps maintain consistency across the application and reduces the risk of typos or mismatches in string literals.
- The comment in Arabic next to `COLLECTION_ASSET_SETTINGS_NAME` indicates that this particular collection is related to file settings, suggesting a specific use case or functionality within the database schema.
```
---

## File: models/enums/ProcessingEnum.py

Summary:
```markdown
The code defines an enumeration class `ProcessingEnum` using Python's `enum` module. This class is used to represent different types of file processing based on file extensions. 

### Class: `ProcessingEnum`
- **Purpose**: To categorize file processing types by their extensions.
- **Members**:
  - `TXT`: Represents a text file with the extension `.txt`.
  - `PDF`: Represents a PDF file with the extension `.pdf`.

### Notes:
- Enums are a way to define a set of named values, which can be used to represent constant values in a more readable and maintainable way.
- The `ProcessingEnum` can be used to check or enforce file types in a program that processes files, ensuring that only supported file types are handled.
```
---

## File: models/enums/ResponseEnums.py

Summary:
```markdown
### Code Summary

The provided code defines an enumeration `ResponseSignal` using Python's `enum` module. This enumeration is used to represent various response signals that can be used within an application, likely for handling file operations, processing tasks, and interactions with a vector database. Each member of the `ResponseSignal` enum is associated with a string that describes a specific state or outcome of an operation.

### Class: `ResponseSignal`

- **Purpose**: To provide a standardized set of response signals for different operations within the application, such as file handling, processing, vector database interactions, and OCR (Optical Character Recognition) tasks.

- **Members**:
  - **File Operations**:
    - `FILE_VALIDATION_SUCCESS`: Indicates successful file validation.
    - `FILE_TYPE_NOT_SUPPORTED`: Indicates an unsupported file type.
    - `FILE_SIZE_TOO_LARGE`: Indicates that the file size is too large.
    - `FILE_UPLOAD_FAILED`: Indicates a failure in file upload.
    - `FILE_UPLOAD_SUCCESS`: Indicates successful file upload.
    - `NO_FILES_ERROR`: Indicates that no files were found.
    - `FILE_ID_ERROR`: Indicates no file found with a given ID.
    - `FILE_NOT_FOUND_ERROR`: Indicates a file was not found.

  - **Processing**:
    - `PROCESSING_FAILED`: Indicates processing failure.
    - `PROCESSING_SUCCESS`: Indicates successful processing.

  - **Vector Database Operations**:
    - `INSERT_INTO_VECTORDB_ERROR`: Indicates an error inserting into the vector database.
    - `INSERT_INTO_VECTORDB_SUCCESS`: Indicates successful insertion into the vector database.
    - `VECTORBD_COLLECTION_RETRIEVED`: Indicates successful retrieval of a vector database collection.
    - `VECTORDB_SEARCH_ERROR`: Indicates an error during a vector database search.
    - `VECTORDB_SEARCH_SUCCESS`: Indicates a successful vector database search.

  - **RAG (Retrieval-Augmented Generation) Operations**:
    - `RAG_ANSWER_ERROR`: Indicates an error in RAG answer generation.
    - `RAG_ANSWER_SUCCESS`: Indicates successful RAG answer generation.

  - **Summarization**:
    - `SUMMARIZATION_SUCCESS`: Indicates successful text summarization.
    - `SUMMARIZATION_ERROR`: Indicates an error in text summarization.

  - **General Errors**:
    - `INVALID_INPUT_ERROR`: Indicates invalid input.
    - `INTERNAL_SERVER_ERROR`: Indicates an internal server error.

  - **OCR Operations**:
    - `OCR_SUCCESS`: Indicates successful OCR processing.
    - `OCR_FAILED`: Indicates OCR processing failure.
    - `OCR_NOT_NEEDED`: Indicates OCR was not needed.
    - `OCR_REPROCESS_SUCCESS`: Indicates successful OCR reprocessing.
    - `OCR_FILE_NOT_FOUND`: Indicates an OCR file was not found.

### Notes

- The enum provides a clear and organized way to manage response signals across different parts of the application, improving code readability and maintainability.
- The comments in Arabic suggest that some signals were newly added for text summarization and error handling.
- This enum can be expanded with additional signals as the application evolves to handle more operations or error states.
```
---

## File: models/enums/__init__.py

Summary:
```markdown
It seems that there is no code provided. Please provide the code you would like me to document, and I will be happy to assist you with it.
```
---

## File: rag_info.txt

Summary:
```markdown
The provided text is a structured and formatted version of a resume in Arabic, designed to be professional and appealing to employers in the field of Artificial Intelligence. Below is a summary of each section:

1. **Personal Information**: This section includes the individual's name, address, phone number, email, and a GitHub link. It provides basic contact and identification details.

2. **Profile**: A brief overview of the individual's academic background and skills, highlighting their specialization in Artificial Intelligence, experience with Python, AI tools, and database management. It also mentions the individual's career aspirations.

3. **Education**: Details about the individual's university education, major in Artificial Intelligence, and expected graduation year.

4. **Work Experience and Projects**: 
   - **Graduation Project**: Describes a project called "ChatPDF," which utilizes AI to analyze and interact with PDF documents, including text summarization, automatic test creation, and chatbot development.
   - **Other Projects**: Mentions a project involving movie classification using NLP techniques.

5. **Skills**: Lists programming skills, AI frameworks, API development, and database management, specifying proficiency levels and tools used.

6. **Languages**: States the individual's language proficiency, with Arabic as the native language and beginner-level English, which is being improved through self-study.

This resume is structured to highlight the individual's technical skills and projects, making it suitable for job applications in the AI field. The offer to format it into a professional PDF suggests a focus on presentation and accessibility for potential employers.
```
---

## File: requirements.txt

Summary:
```markdown
This document is a `requirements.txt` file, which specifies the dependencies for a Python project. It lists the required packages along with their versions or version constraints. Here's a summary of the key components:

### Key Libraries and Their Purpose:

1. **FastAPI (0.115.6)**: A modern, fast (high-performance) web framework for building APIs with Python 3.7+ based on standard Python type hints.

2. **Uvicorn (0.34.0)**: A lightning-fast ASGI server implementation, using `uvloop` and `httptools`.

3. **Python-Multipart (0.0.20)**: A streaming multipart parser for Python, useful for handling file uploads in web applications.

4. **Python-Dotenv (1.0.1)**: Reads key-value pairs from a `.env` file and can set them as environment variables.

5. **Pydantic-Settings (2.7.1)**: Provides settings management using Pydantic for data validation and settings management.

6. **Aiofiles (24.1.0)**: A library for handling file operations asynchronously.

7. **Langchain (0.3.14) & Langchain-Community (0.3.14)**: Libraries for building applications with language models, such as OpenAI's GPT.

8. **PyMuPDF (1.25.1)**: A Python binding for MuPDF, a lightweight PDF and XPS viewer.

9. **Motor (3.6.1)**: An asynchronous driver for MongoDB, built on top of `pymongo`.

10. **OpenAI (1.61.0) & Cohere (5.13.11)**: SDKs for interacting with OpenAI and Cohere's language models.

11. **Qdrant-Client (1.13.2), Milvus (2.3.5), Weaviate-Client (4.11.0)**: Clients for vector databases, useful for storing and querying high-dimensional vectors.

12. **Orjson (3.10.15)**: A fast, correct JSON library for Python.

13. **Scikit-Learn**: A machine learning library for Python, providing simple and efficient tools for data mining and data analysis.

14. **Google-GenerativeAI (>=0.8.4)**: A library for interacting with Google's generative AI models.

### Additional Libraries:

- **h11, pip, attrs, distro, protobuf, cryptography**: Libraries for handling HTTP, package management, data serialization, and security.
  
- **Filelock, PyYAML, Typing-Extensions, Sniffio, Click**: Utilities for file locking, YAML parsing, type hinting, asynchronous I/O, and command-line interfaces.

- **Uvloop, Certifi, Starlette, Pydantic, Pydantic-Core**: Libraries for event loops, SSL certificates, web frameworks, and data validation.

- **Requests, Websockets, Anyio, H2, Urllib3**: Libraries for HTTP requests, WebSocket communication, asynchronous I/O, HTTP/2, and URL handling.

- **HttpTools, Watchfiles, Annotated-Types, Idna, Charset-Normalizer**: Utilities for HTTP parsing, file watching, type annotations, international domain names, and character encoding.

- **Httpx, Pymongo, Tenacity, Numpy**: Libraries for HTTP requests, MongoDB interaction, retrying operations, and numerical computations.

- **Langchain-Text-Splitters (0.3.5)**: A utility for splitting text, likely used in conjunction with language models.

- **Pdfplumber (0.11.6)**: A library for extracting text, tables, and metadata from PDFs.

### Notes:
- The use of `~=` indicates compatibility with a specific version, allowing for minor updates.
- The file includes both specific version requirements and version ranges, providing flexibility for some dependencies while ensuring stability for others.
- This setup is likely for a project involving web development, AI model integration, and data processing.
```
---

## File: routes/__init__.py

Summary:
```markdown
This code is an initialization file for a Python package, typically named `__init__.py`. It is used to set up the package's namespace and make it easier to import modules from the package. This specific `__init__.py` file is part of a package named `routes` and it imports several modules from the same package. Here's a breakdown of the components:

### Imported Modules:
- **base**: This module is imported from the `routes` package. It likely contains foundational routing logic or base classes used by other modules in the package.
- **data**: This module may handle data-related operations, such as data retrieval, storage, or manipulation.
- **nlp**: This module is likely responsible for natural language processing tasks, possibly including text analysis or language understanding.
- **integrated**: This module might integrate various components or functionalities, possibly serving as a bridge between different parts of the application.
- **asset_settings_router**: This module is probably responsible for routing related to asset settings, managing configurations or settings for assets within the application.
- **summarization**: This module likely deals with text summarization tasks, providing functionalities to condense text content.

### Purpose:
The purpose of this `__init__.py` file is to make these modules readily available when the `routes` package is imported. By importing these modules here, users of the package can access them directly from the `routes` namespace, simplifying the import statements in other parts of the application.

### Notes:
- This file does not contain any executable code or functions itself; it serves purely as an organizational tool for the package.
- The actual functionality and implementation details of each module would be defined in their respective files within the `routes` directory.
- The presence of this file indicates that `routes` is intended to be used as a package, which is a collection of modules that can be imported as a single unit.
```
---

## File: routes/asset_settings_router.py

Summary:
```markdown
This code defines a FastAPI router for managing asset settings, providing endpoints to retrieve and update settings for specific assets, as well as to list available models from different providers. Here's a breakdown of the components:

### Data Models
- **AssetSettingsUpdate**: A Pydantic model for updating asset settings, containing a dictionary of settings.
- **AvailableModelsResponse**: A Pydantic model for the response containing a list of available models.
- **AssetSettingsResponse**: A Pydantic model for the response containing asset settings and available providers.

### Router Configuration
- **asset_settings_router**: An instance of `APIRouter` with the prefix `/api/v1/asset-settings` and tags `["api_v1", "asset_settings"]`.

### Constants
- **AVAILABLE_PROVIDERS**: A list of dictionaries representing available providers, each with an `id` and `name`.
- **EDITABLE_SETTINGS**: A list of settings keys that are allowed to be edited.

### Endpoints
1. **GET /{asset_id}**: Retrieves the settings for a specific asset.
   - Checks if the asset exists using `AssetModel`.
   - Retrieves settings using `AssetSettingsModel`.
   - Returns the settings and available providers.

2. **PUT /{asset_id}**: Updates the settings for a specific asset.
   - Validates the existence of the asset.
   - Checks if the provided settings are editable.
   - Updates the settings using `AssetSettingsModel`.
   - Returns a success message and the asset ID if successful.

3. **GET /models/{provider}**: Retrieves available models for a specific provider.
   - Validates if the provider is available.
   - Retrieves models using `AssetSettingsModel`.
   - Returns the list of models.

### Notes
- The endpoints utilize asynchronous functions to handle requests.
- The code uses `HTTPException` to handle errors such as asset not found or invalid settings.
- The `AssetModel` and `AssetSettingsModel` are assumed to be custom models handling database interactions.
- The `ResponseSignal` is used to provide standardized response signals, indicating success or failure.
- The code includes comments in Arabic, providing additional context and explanations.
```
---

## File: routes/base.py

Summary:
```markdown
This code defines a FastAPI application with a single endpoint using an `APIRouter`. The router is configured with a prefix and a tag for versioning and documentation purposes.

### Key Components:

- **`base_router`**: An instance of `APIRouter` with a prefix `/api/v1` and a tag `api_v1`. This router is used to group related endpoints under a common path and tag.

- **`welcome` Endpoint**: 
  - **Path**: `/api/v1/`
  - **Method**: `GET`
  - **Description**: This asynchronous function serves as a welcome endpoint. It returns a JSON response containing the application's name and version.
  - **Parameters**: 
    - `app_settings`: An instance of `Settings` which is automatically provided by FastAPI's dependency injection system using the `Depends` function. It retrieves application settings from the environment.
  - **Returns**: A dictionary with keys `app_name` and `app_version`, which are fetched from the `app_settings`.

### Dependencies:

- **`get_settings`**: A function imported from `helpers.config` that provides the application settings.
- **`Settings`**: A class imported from `helpers.config` that likely defines the structure of the application settings, including attributes like `APP_NAME` and `APP_VERSION`.

### Usage Notes:

- The `welcome` endpoint utilizes FastAPI's dependency injection to access configuration settings, which are expected to be defined in the environment and structured by the `Settings` class.
- The use of `APIRouter` allows for modular and organized routing, which is beneficial for larger applications that may have multiple versions or sections of an API.
```
---

## File: routes/data.py

Summary:
```markdown
This code defines a FastAPI application with endpoints for uploading, processing, and retrieving files and their associated data chunks. It includes several routes and functions, each serving a specific purpose:

1. **Logging and Configuration**: 
   - Sets up a logger for error logging.
   - Uses dependency injection to get application settings.

2. **Data Router**: 
   - An `APIRouter` is created with the prefix `/api/v1/data` for handling data-related requests.

3. **Upload Data Endpoint** (`/upload`):
   - Accepts file uploads and optionally processes them.
   - Parameters:
     - `file`: The file to be uploaded.
     - `process`: Boolean indicating whether to process the file after upload.
     - `chunk_size`: Size of chunks for processing.
     - `overlap_size`: Overlap size between chunks.
   - Validates the file, saves it to a unique path, and stores metadata in a database.
   - If processing is requested, it processes the file into chunks and stores these in the database.

4. **Get Files Endpoint** (`/files`):
   - Retrieves a paginated list of uploaded files.
   - Parameters:
     - `page`: Page number for pagination.
     - `page_size`: Number of files per page.

5. **Process File Endpoint** (`/process/{file_id}`):
   - Processes a specific file by its ID.
   - Parameters:
     - `file_id`: The ID of the file to process.
     - `chunk_size`: Size of chunks for processing.
     - `overlap_size`: Overlap size between chunks.
   - Deletes existing chunks for the file, processes the file content, and stores new chunks.

6. **Get Chunks Endpoint** (`/chunks/{asset_id}`):
   - Retrieves text chunks for a specific file, optionally with adjacent chunks for context.
   - Parameters:
     - `asset_id`: The ID of the file.
     - `page`: Page number for pagination.
     - `page_size`: Number of chunks per page.
     - `with_context`: Boolean indicating whether to include adjacent chunks.

7. **Helper Function `get_adjacent_chunks`**:
   - Retrieves adjacent chunks for a given chunk to provide additional context.
   - Parameters:
     - `chunk_model`: The model for interacting with chunk data.
     - `asset_id`: The ID of the file.
     - `current_order`: The order of the current chunk.
     - `context_size`: Number of adjacent chunks to retrieve in each direction.

This code is structured to handle file uploads, validate and store them, process them into manageable chunks, and provide access to these chunks with optional context. It uses asynchronous operations to handle file I/O and database interactions efficiently.
```
---

## File: routes/file.py

Summary:
```markdown
This code defines a FastAPI endpoint for a unified processing workflow involving file upload, data processing, and indexing for a given project. It includes the following components:

### Class: `UnifiedProcessRequest`
- **Purpose**: Defines the request model for the unified process endpoint.
- **Attributes**:
  - `file`: An `UploadFile` object representing the file to be processed.
  - `chunk_size`: An integer specifying the size of data chunks for processing.
  - `overlap_size`: An integer specifying the overlap size between data chunks.
  - `do_reset_processing`: An integer flag to indicate if the processing should be reset (default is 0).
  - `do_reset_index`: An integer flag to indicate if the indexing should be reset (default is 0).

### Endpoint: `unified_process`
- **Path**: `/unified-process/{project_id}`
- **Method**: POST
- **Parameters**:
  - `request`: The HTTP request object.
  - `project_id`: A string identifier for the project.
  - `unified_request`: An instance of `UnifiedProcessRequest`.
  - `app_settings`: Application settings, injected via dependency (`Depends(get_settings)`).
- **Workflow**:
  1. **Project Verification**: Checks if the project exists or creates a new one if it doesn't.
  2. **File Upload**: Uploads the file and retrieves a file ID.
  3. **Data Processing**: Processes the uploaded file using specified chunk and overlap sizes.
  4. **Indexing**: Indexes the processed data.
  5. **Response**: Returns a JSON response indicating success or failure, including details of the uploaded file, processed chunks, and indexed items.

### Error Handling
- Catches exceptions during the process and logs errors.
- Returns a 500 status code with an error message if any step fails.

### Notes
- The code includes commented-out helper functions for resource cleanup, which can be used to revert changes in case of errors.
- The code relies on external controllers and models (`ProjectController`, `NLPController`, `ChunkModel`, etc.) for various operations, indicating a modular design.

This setup is suitable for applications requiring structured data processing and indexing workflows, particularly in projects involving natural language processing (NLP).
```
---

## File: routes/integrated.py

Summary:
```markdown
This code defines a FastAPI application with an API router for handling file uploads, processing, and indexing, with added support for Optical Character Recognition (OCR) for PDF files. The main features are implemented in three endpoints:

1. **`upload_process_push`**: This endpoint allows users to upload a file, optionally process it with OCR if it's a PDF, split the file into chunks, and index these chunks in a vector database. It includes parameters for chunk size, overlap size, and OCR settings. The process is divided into four stages: file upload, optional OCR processing, file processing, and vector database indexing.

2. **`get_ocr_status`**: This endpoint retrieves the OCR processing status for a specific file. It checks if the file exists in the database and then returns the OCR status.

3. **`reprocess_with_ocr`**: This endpoint allows reprocessing of an existing PDF file with OCR. It checks if the file is a PDF and then performs OCR with specified language settings.

### Key Components:

- **`DataController`**: Manages file validation and unique file path generation.
- **`ProcessController`**: Handles file content reading and processing into chunks.
- **`NLPController`**: Manages indexing of file chunks in a vector database.
- **`OCRController`**: Handles OCR processing of PDF files.
- **`AssetModel` and `ChunkModel`**: ORM models for managing file and chunk records in the database.
- **`PushRequest`**: A data model for handling push requests with options like resetting the index.

### Parameters:

- **`file`**: The file to be uploaded and processed.
- **`chunk_size`**: The size of each chunk for processing, default is 1500 characters.
- **`overlap_size`**: The overlap size between chunks, default is 150 characters.
- **`do_rest`**: A boolean to determine if the index should be reset, default is `False`.
- **`enable_ocr`**: A boolean to enable OCR processing for PDFs, default is `True`.
- **`ocr_languages`**: A string specifying languages for OCR, default is `"ara+eng"`.

### Error Handling:

- The code logs errors using a logger set up for the `uvicorn.error` channel.
- It returns JSON responses with appropriate HTTP status codes and error messages for different failure points, such as file validation, upload, processing, and OCR errors.

This setup is suitable for applications that require automated processing of uploaded documents, especially those needing text extraction from PDFs via OCR.
```
---

## File: routes/nlp.py

Summary:
```markdown
This code defines a FastAPI router for handling NLP-related operations, specifically for indexing, searching, and retrieving information from a vector database using files. The router is set up with a prefix `/api/v1/nlp` and includes several endpoints for interacting with the database.

### Key Components:

1. **Router Setup**:
   - `nlp_router`: An instance of `APIRouter` with the prefix `/api/v1/nlp` and tagged as `api_v1` and `nlp`.

2. **Endpoints**:
   - **`/index/push/{file_id}`**: 
     - **Method**: POST
     - **Function**: `index_file`
     - **Description**: Indexes a file into the vector database. It retrieves the file's asset and its chunks, then indexes these chunks using `NLPController`. If successful, it returns the number of inserted items.
     - **Parameters**: 
       - `file_id`: ID of the file to be indexed.
       - `push_request`: An instance of `PushRequest` containing additional indexing instructions.
   
   - **`/index/info/{file_id}`**:
     - **Method**: GET
     - **Function**: `get_file_index_info`
     - **Description**: Retrieves index information for a specific file.
     - **Parameters**: 
       - `file_id`: ID of the file whose index information is requested.

   - **`/index/search/{file_id}`**:
     - **Method**: POST
     - **Function**: `search_index`
     - **Description**: Searches the indexed data of a file for a given text query.
     - **Parameters**: 
       - `file_id`: ID of the file to search within.
       - `search_request`: An instance of `SearchRequest` containing the search query and limit.

   - **`/index/answer/{file_id}`**:
     - **Method**: POST
     - **Function**: `get_answer`
     - **Description**: Retrieves an answer using a RAG (Retrieval-Augmented Generation) system based on the indexed file data.
     - **Parameters**: 
       - `file_id`: ID of the file to use for generating an answer.
       - `search_request`: An instance of `SearchRequest` for the query.

   - **`/index/chat/{file_id}`**:
     - **Method**: GET
     - **Function**: `get_chat_history`
     - **Description**: Retrieves the chat history associated with a specific file.
     - **Parameters**: 
       - `file_id`: ID of the file whose chat history is requested.

### Additional Notes:
- The code uses `NLPController` to handle interactions with the vector database, including indexing, searching, and answering queries.
- The code handles errors by returning appropriate JSON responses with error signals.
- Logging is set up using Python's `logging` module to track errors and important events.
- The code includes commented-out sections for potential future enhancements, such as real-time updates using Socket.io for indexing and answering processes.
```
---

## File: routes/schems/__init__.py

Summary:
```markdown
It seems that you've submitted an empty prompt. Please provide the code you would like me to document, and I'll be happy to assist!
```
---

## File: routes/schems/data.py

Summary:
```markdown
The code defines a Pydantic model named `ProcessRequest` that is used to represent a request for processing a file. This model includes the following attributes:

- `file_id` (str): A required string attribute that represents the unique identifier of the file to be processed. It is initialized with a default value of `None`.

- `chunk_size` (Optional[int]): An optional integer attribute that specifies the size of the chunks into which the file should be divided for processing. It has a default value of 100.

- `overlap_size` (Optional[int]): An optional integer attribute that indicates the size of the overlap between consecutive chunks. This is useful for ensuring continuity between chunks. It has a default value of 20.

- `do_rest` (Optional[int]): An optional integer attribute that might be used to specify whether the rest of the file should be processed after initial processing. It has a default value of 0, indicating no further processing by default.

### Notes:
- The `ProcessRequest` class inherits from `BaseModel`, which is part of the Pydantic library. Pydantic models provide data validation and settings management using Python type annotations.
- The use of `Optional` from the `typing` module indicates that the attributes can be set to `None`, although they have default values provided.
- This model can be used to easily validate and manage data related to file processing requests, ensuring that the input data conforms to the expected types and constraints.
```
---

## File: routes/schems/nlp.py

Summary:
```markdown
The provided code defines three data models using the Pydantic library, which is used for data validation and settings management using Python type annotations. Each model represents a different type of request and includes specific fields with default values and types.

### Classes:

1. **PushRequest**:
   - Represents a request to push data.
   - **Attributes**:
     - `do_rest` (Optional[bool]): A boolean flag indicating whether to perform a REST operation. Defaults to `False`.

2. **SearchRequest**:
   - Represents a request to search for text.
   - **Attributes**:
     - `text` (str): The text to search for.
     - `limit` (Optional[int]): The maximum number of search results to return. Defaults to `10`.

3. **SummarizationRequest**:
   - Represents a request to summarize text.
   - **Attributes**:
     - `text` (str): The text that needs to be summarized.
     - `language` (str): The language of the text, with a default value of "en" for English.
     - `max_output_tokens` (int): The maximum number of tokens allowed in the summary. Defaults to `300`.

### Notes:
- The `PushRequest` class uses a boolean default value (`False`) for the `do_rest` attribute instead of an integer (`0`), which is more semantically appropriate for a boolean flag.
- The `SearchRequest` and `SummarizationRequest` classes include optional parameters with default values, allowing for flexibility in the request configurations.
- The use of Pydantic's `BaseModel` ensures that input data is validated against the specified types and constraints.
```
---

## File: routes/schems/summarization.py

Summary:
```markdown
This code defines three Pydantic models for handling summarization requests and responses in a structured format. These models are used to validate and manage data related to text summarization processes.

### Classes and Their Descriptions:

1. **SummarizationRequest**:
   - Represents a request for text summarization.
   - **Attributes**:
     - `language`: Specifies the desired language for the summary (e.g., 'ar' for Arabic, 'en' for English).
     - `chunk_size`: Defines the size of each text chunk to be summarized.
     - `overlap_size`: Specifies the overlap between consecutive text chunks.
     - `force_regenerate`: A boolean indicating whether to regenerate the summary if it already exists.
     - `detail_level`: Determines the level of detail in the summary (options: 'brief', 'balanced', 'detailed').

2. **SummarizationResponse**:
   - Represents the response after text summarization is completed.
   - **Attributes**:
     - `file_id`: A unique identifier for the summarized file.
     - `final_summary`: The final summarized text.
     - `language`: The language used for the summary.
     - `total_pages`: The total number of pages processed.
     - `total_chunks`: The total number of text chunks processed.
     - `summary_length`: The length of the final summary.
     - `processing_time`: Time taken to process the summarization in seconds.
     - `quality_score`: A score representing the quality of the summary.
     - `metadata`: Additional metadata related to the summarization process.
     - `created_at`: Timestamp indicating when the summary was created.

3. **BatchSummarizationRequest**:
   - Used for handling batch summarization requests.
   - **Attributes**:
     - `language`: Specifies the language for the summaries.
     - `chunk_size`: Defines the size of each text chunk.
     - `overlap_size`: Specifies the overlap between chunks.
     - `detail_level`: Determines the level of detail in the summaries.

### Notes:
- Each class includes a `Config` inner class with an `example` dictionary to provide usage examples.
- The models use Pydantic's `Field` to define attributes with descriptions and default values, ensuring data validation and clarity.
- The `SummarizationResponse` includes a `metadata` field for additional information, which can be customized as needed.

These models are essential for applications that require structured input and output for text summarization tasks, ensuring consistency and ease of integration.
```
---

## File: routes/summarization.py

Summary:
```markdown
This code defines a FastAPI router for handling summarization-related API endpoints. It includes several endpoints for summarizing files, retrieving summarization status, getting summarization chunks, and deleting summarizations. The router is prefixed with `/api/v1/summarization` and tagged as "Summarization".

### Classes

- **FlexibleSummarizationRequest**: A Pydantic model for incoming summarization requests. It includes:
  - `language`: Required; specifies the language for summarization (e.g., "ar" for Arabic, "en" for English).
  - `chunk_size`: Optional; defines the size of each text chunk (default is 1500, range 500-3000).
  - `overlap_size`: Optional; defines the overlap between chunks (default is 150, range 50-500).

- **DetailedSummarizationResponse**: A Pydantic model for the response of a summarization request. It includes:
  - `file_id`: The ID of the file being summarized.
  - `summary`: The resulting summary text.
  - `language`: The language used for summarization.
  - `total_pages`: Total number of pages processed.
  - `total_chunks`: Total number of chunks processed.
  - `processing_time`: Time taken to process the summarization.
  - `summary_length`: Length of the summary text.

### Endpoints

- **POST /{file_id}**: Summarizes a file with customizable parameters. It requires a `FlexibleSummarizationRequest` and returns a `DetailedSummarizationResponse`. It checks for the presence of a database client and summarization provider in the application state.

- **GET /{file_id}**: Retrieves the current status of a summarization process for a given file ID.

- **GET /{file_id}/chunks**: Retrieves the individual chunks of the summarization for a given file ID.

- **DELETE /{file_id}**: Deletes the summarization and its associated chunks for a given file ID.

### Error Handling

- Uses `HTTPException` to handle various errors such as missing database client, missing summarization provider, file not found, and general exceptions. The exceptions provide appropriate HTTP status codes and error messages.

### Notes

- The code assumes the existence of a `SummarizationController` class with methods for summarizing files, getting summarization status, and managing summary chunks.
- The `summarization_router` is designed to be integrated into a larger FastAPI application, where the application state is expected to include a database client and summarization provider.
- The code includes Arabic comments and descriptions, indicating it might be intended for an Arabic-speaking audience or application.
```
---

## File: stores/Summarization/SummarizationEnums.py

Summary:
```markdown
This code defines several enumerations (enums) related to summarization services, strategies, and errors. Each enum class provides a set of predefined constants that represent various options or categories within a specific context.

### Classes and Enums:

1. **SummarizationProvider**: Represents different summarization service providers.
   - `OPENAI`: Represents the OpenAI service.
   - `COHERE`: Represents the Cohere service.
   - `GEMINI`: Represents the Gemini service.
   - `DEEPSEEK`: Represents the DeepSeek service.

2. **SummarizationStrategy**: Represents different strategies for summarization.
   - `EXTRACTIVE`: Extractive summarization, which involves selecting parts of the text.
   - `ABSTRACTIVE`: Abstractive summarization, which involves generating new text.
   - `HYBRID`: A combination of extractive and abstractive summarization.

3. **MergeStrategy**: Represents strategies for merging summaries.
   - `SEQUENTIAL`: Sequential merging of summaries.
   - `THEMATIC`: Thematic merging based on content themes.
   - `HIERARCHICAL`: Hierarchical merging based on structure.

4. **QualityMetric**: Represents metrics for evaluating the quality of a summary.
   - `COMPLETENESS`: Measures how complete the summary is.
   - `ACCURACY`: Measures the accuracy of the summary.
   - `CONCISENESS`: Measures how concise the summary is.
   - `COHERENCE`: Measures the coherence of the summary.
   - `READABILITY`: Measures the readability of the summary.

5. **SummarizationError**: Represents potential errors that can occur during summarization.
   - `PROVIDER_ERROR`: Error related to the summarization provider.
   - `INVALID_INPUT`: Error due to invalid input.
   - `PROCESSING_ERROR`: Error during the processing of the summary.
   - `QUALITY_ERROR`: Error related to the quality of the summary.
   - `MERGE_ERROR`: Error during the merging of summaries.

6. **LanguageCode**: Represents supported language codes.
   - `ARABIC`: Arabic language code.
   - `ENGLISH`: English language code.
   - `FRENCH`: French language code.
   - `SPANISH`: Spanish language code.
   - `GERMAN`: German language code.

### Notes:
- The enums are defined using Python's `Enum` class, which allows for the creation of symbolic names bound to unique, constant values.
- The code includes Arabic comments alongside the English descriptions, indicating multilingual support or usage context.
- These enums can be used to standardize inputs and outputs in a summarization system, ensuring consistent handling of providers, strategies, errors, and language codes.
```
---

## File: stores/Summarization/SummarizationInterface.py

Summary:
```markdown
The provided code defines an abstract base class `SummarizationInterface` using Python's `abc` module. This interface is designed for summarization providers and outlines the necessary methods that any implementing class must define. The interface is structured to support asynchronous operations and includes methods for initialization, text summarization, summary merging, input validation, and provider information retrieval. Each method is documented in Arabic, specifying its purpose, parameters, and expected return values.

### Class and Method Documentation

- **Class: `SummarizationInterface`**
  - An abstract base class that serves as a unified interface for different summarization providers. It defines the essential methods that any summarization provider must implement.

- **Method: `initialize(self, config: Dict[str, Any]) -> None`**
  - Asynchronously initializes the summarization provider with the necessary configuration settings.
  - **Parameters:**
    - `config`: A dictionary containing provider-specific settings.

- **Method: `summarize_chunk(self, text: str, page_number: int = None, context: Dict[str, Any] = None, language: str = None) -> str`**
  - Asynchronously summarizes a single chunk of text.
  - **Parameters:**
    - `text`: The text to be summarized.
    - `page_number`: Optional page number for reference.
    - `context`: Optional additional context for summarization.
    - `language`: Optional language specification for the summary.
  - **Returns:** A string containing the generated summary.

- **Method: `merge_summaries(self, summaries: List[Dict[str, Any]], target_language: str = None) -> str`**
  - Asynchronously merges multiple summaries into a single summary.
  - **Parameters:**
    - `summaries`: A list of summaries to be merged.
    - `target_language`: Optional target language for the merged summary.
  - **Returns:** A string containing the merged summary.

- **Method: `validate_input(self, text: str) -> bool`**
  - Validates the input text to ensure it meets the necessary criteria for summarization.
  - **Parameters:**
    - `text`: The text to validate.
  - **Returns:** A boolean indicating whether the text is valid.

- **Method: `get_provider_info(self) -> Dict[str, Any]`**
  - Retrieves information about the summarization provider.
  - **Returns:** A dictionary containing provider information.

This interface is designed to be extended by concrete classes that implement the specific logic for different summarization providers. Each method is expected to be overridden with provider-specific functionality.
```
---

## File: stores/Summarization/SummarizationProviderFactory.py

Summary:
```markdown
The `SummarizationProviderFactory` class is responsible for creating and managing summarization providers. It supports multiple providers and provides methods to create, validate, and retrieve information about these providers. The factory design pattern is used to encapsulate the instantiation logic of different summarization providers.

### Key Components:

- **Initialization (`__init__`)**: 
  - Takes a configuration dictionary (`config`) as an argument.
  - Sets up a logger and a dictionary `_providers` that maps provider names to their respective classes.

- **`create(provider_name: str) -> Optional[SummarizationInterface]`**:
  - Attempts to create an instance of a summarization provider based on the `provider_name`.
  - Logs errors if the provider is unsupported or if instantiation fails.
  - Returns an instance of `SummarizationInterface` or `None` if creation fails.

- **`_get_provider_config(provider_name: str) -> Dict[str, Any]`**:
  - Retrieves specific configuration settings for a given provider.
  - Supports different configurations for each provider such as API keys and model settings.

- **`get_available_providers() -> List[str]`**:
  - Returns a list of available provider names.

- **`is_provider_available(provider_name: str) -> bool`**:
  - Checks if a given provider is available in the `_providers` dictionary.

- **`validate_provider_config(provider_name: str) -> bool`**:
  - Validates the configuration for a specific provider, primarily checking for the presence of API keys.

- **`get_provider_info(provider_name: str) -> Dict[str, Any]`**:
  - Provides detailed information about a specific provider, including supported models, languages, and costs.

- **`create_with_fallback(primary_provider: str, fallback_providers: List[str]) -> Optional[SummarizationInterface]`**:
  - Attempts to create the primary provider and falls back to alternative providers if the primary fails.
  - Logs warnings and errors appropriately.

### Notes:
- The factory currently supports the OpenAI provider and placeholders for Cohere and Gemini providers.
- The configuration settings are expected to be part of the `config` object, which should include API keys and model settings.
- The class is designed to be extendable, allowing for additional providers to be added easily by modifying the `_providers` dictionary and `_get_provider_config` method.
- Logging is used extensively to track the creation process and any errors encountered.
```
---

## File: stores/Summarization/SummaryMerger.py

Summary:
```markdown
The `SummaryMerger` class is designed to merge multiple summaries into a coherent single summary using different strategies. It supports sequential, thematic, and hierarchical merging strategies and includes methods for sorting, merging, and enhancing summaries. Here's a breakdown of its components:

### Class: `SummaryMerger`
- **Purpose**: To merge multiple summaries into one coherent summary.
- **Logging**: Uses a logger to record the merging process and errors.

### Method: `merge_summaries`
- **Parameters**:
  - `summaries`: A list of dictionaries containing summaries to be merged.
  - `merge_strategy`: A string specifying the merging strategy ("sequential", "thematic", "hierarchical").
- **Returns**: A single merged summary as a string.
- **Functionality**: Sorts summaries, applies the chosen merging strategy, and enhances the final result.

### Method: `_sort_summaries`
- **Parameters**: `summaries` - List of summaries to sort.
- **Returns**: Sorted list based on page number and chunk order.

### Method: `_merge_sequential`
- **Parameters**: `summaries` - List of summaries to merge sequentially.
- **Returns**: Merged summary text.
- **Functionality**: Merges summaries in order, removing redundancy.

### Method: `_merge_thematic`
- **Parameters**: `summaries` - List of summaries to merge thematically.
- **Returns**: Merged summary text.
- **Functionality**: Groups summaries by themes and merges them.

### Method: `_merge_hierarchical`
- **Parameters**: `summaries` - List of summaries to merge hierarchically.
- **Returns**: Merged summary text.
- **Functionality**: Groups summaries by hierarchy levels and merges them.

### Method: `_identify_themes`
- **Parameters**: `summaries` - List of summaries.
- **Returns**: Dictionary of themes with associated summaries.
- **Functionality**: Identifies themes based on keywords.

### Method: `_identify_hierarchy_levels`
- **Parameters**: `summaries` - List of summaries.
- **Returns**: Dictionary of hierarchy levels with associated summaries.
- **Functionality**: Determines hierarchy levels based on text patterns.

### Method: `_remove_redundancy`
- **Parameters**: `new_text`, `existing_text`.
- **Returns**: Text with redundancy removed.
- **Functionality**: Removes duplicate sentences from new text.

### Method: `_calculate_similarity`
- **Parameters**: `text1`, `text2`.
- **Returns**: Similarity score between two texts.
- **Functionality**: Calculates lexical similarity.

### Method: `_enhance_final_merge`
- **Parameters**: `merged_text`.
- **Returns**: Enhanced merged text.
- **Functionality**: Improves formatting, transitions, and coherence.

### Method: `get_merge_statistics`
- **Parameters**: `original_summaries`, `merged_summary`.
- **Returns**: Dictionary of statistics about the merging process.

### Method: `validate_merge_quality`
- **Parameters**: `original_summaries`, `merged_summary`.
- **Returns**: Dictionary of quality metrics and issues.

### Method: `_calculate_redundancy_level`
- **Parameters**: `text`.
- **Returns**: Redundancy level of the text.
- **Functionality**: Measures redundancy based on sentence similarity.

### Method: `_calculate_coherence_score`
- **Parameters**: `text`.
- **Returns**: Coherence score of the text.
- **Functionality**: Evaluates coherence based on paragraph similarity.

This class is useful for applications that need to consolidate multiple text summaries into a single, coherent document, with options for different organizational strategies.
```
---

## File: stores/Summarization/SummaryQualityChecker.py

Summary:
```markdown
The `SummaryQualityChecker` class is designed to evaluate the quality of text summaries. It provides a comprehensive analysis of a summary's quality based on various criteria such as completeness, accuracy, conciseness, coherence, and readability. The class includes methods for checking basic quality, structure, language quality, coherence, and accuracy against an original text if available. It also generates recommendations for improving the summary.

### Key Components:

- **Initialization**: Sets up logging and defines quality thresholds for different aspects of summary quality.

- **check_quality**: Main method to evaluate the quality of a summary. It performs various checks and compiles a quality report with scores, recommendations, and metadata.

- **_basic_quality_checks**: Evaluates basic aspects like length appropriateness, vocabulary diversity, and information density.

- **_check_length**: Assesses if the summary's length is within an ideal range.

- **_check_vocabulary_diversity**: Measures the diversity of vocabulary used in the summary.

- **_check_information_density**: Evaluates the density of important information within the summary.

- **_check_structure**: Checks the structure and formatting of the summary, including paragraph and sentence structure.

- **_check_paragraphs**: Analyzes paragraph structure for length and variety.

- **_check_sentences**: Assesses sentence structure, length, and variety.

- **_check_formatting**: Evaluates general formatting, including spacing and punctuation.

- **_check_language_quality**: Checks grammar, spelling, and punctuation quality.

- **_check_basic_grammar**: Looks for common grammatical errors.

- **_check_common_spelling_errors**: Identifies common spelling mistakes.

- **_check_punctuation**: Evaluates the use of punctuation marks.

- **_check_coherence**: Assesses lexical coherence, logical flow, and sentence connection.

- **_check_lexical_coherence**: Measures lexical similarity between consecutive sentences.

- **_check_logical_flow**: Evaluates logical flow using indicators like order and contrast.

- **_check_sentence_connection**: Checks for connectors that link sentences.

- **_check_accuracy**: Compares the summary with the original text for key information preservation, numbers accuracy, and hallucination (adding information not present in the original).

- **_check_key_information_preservation**: Ensures key information from the original text is retained.

- **_check_numbers_accuracy**: Verifies the accuracy of numbers and statistics.

- **_check_no_hallucination**: Ensures no external information is added to the summary.

- **_calculate_overall_score**: Computes an overall quality score based on weighted criteria.

- **_generate_recommendations**: Provides suggestions for improving the summary based on detailed scores.

- **check_readability**: Evaluates the readability of the text, considering sentence length and vocabulary complexity.

- **generate_detailed_report**: Produces a detailed report combining quality checks, readability, and additional information like word count and presence of numbers.

### Usage Notes:
- The class is designed to handle Arabic text, as indicated by the use of Arabic comments and patterns.
- The quality checks are modular, allowing for easy extension or modification of individual checks.
- The class can be used to evaluate summaries in educational, professional, or research contexts where summary quality is critical.
```
---

## File: stores/Summarization/providers/OpenAISummarizationProvider.py

Summary:
```markdown
The `OpenAISummarizationProvider` class is an implementation of a summarization service using OpenAI's API, with enhanced templates for generating summaries. It extends the `SummarizationInterface` and provides methods to initialize the provider, summarize text chunks, and merge multiple summaries into a single one. The class is designed to handle different content types and languages, with a focus on Arabic and English.

### Key Components:

- **Initialization**: 
  - `__init__`: Sets up default parameters like model name, temperature, max tokens, and language. Initializes logging and a flag to check if the provider is initialized.
  - `initialize`: Asynchronously initializes the OpenAI client using provided configuration, tests the connection, and sets the initialized flag.

- **Summarization**:
  - `summarize_chunk`: Asynchronously summarizes a given text chunk using enhanced templates. It detects content type if not specified and prepares a prompt for the OpenAI API. It processes the response to produce a refined summary.
  - `_detect_content_type`: Determines the content type of the text (general, technical, academic, medical, legal) based on keyword indicators.

- **Merging Summaries**:
  - `merge_summaries`: Asynchronously merges multiple summaries into one using enhanced templates. It prepares the text for merging, sends a request to OpenAI, and processes the response.
  - `_prepare_enhanced_merge_prompt`: Prepares the prompt for merging summaries, including additional context if provided.

- **Prompt and Post-Processing**:
  - `_get_enhanced_system_prompt`: Generates a system prompt based on language and content type.
  - `_get_merge_system_prompt`: Generates a system prompt for merging summaries.
  - `_post_process_summary` and `_post_process_final_summary`: Refine the summaries by removing unwanted phrases and improving formatting.

- **Validation and Information**:
  - `validate_input`: Checks if the input text is valid based on length constraints.
  - `get_provider_info`: Returns metadata about the provider, including supported features and configuration.

### Notes:
- The class uses enhanced templates for prompts, which are imported from external modules.
- It supports both Arabic and English languages, with specific prompt adjustments for each language.
- The provider is designed to handle different content types, ensuring that summaries are contextually accurate and maintain important details.
- The class includes logging for debugging and tracking the summarization process.
```
---

## File: stores/Summarization/providers/__init__.py

Summary:
```markdown
The provided code is missing. Please provide the code you would like documented, and I will be happy to assist you with a summary and explanation.
```
---

## File: stores/llm/LLMEnums.py

Summary:
```markdown
This code defines several enumeration classes using Python's `enum` module. These classes are used to represent various constants related to language model interactions, specifically for different AI platforms such as OpenAI, Cohere, Gemini, and DeepSeek. Enumerations are useful for defining a set of named values that can be used to represent different states or options in a clear and type-safe manner.

### Classes and Enums:

1. **LLMEnums**:
   - Represents the supported language model types.
   - Members:
     - `OPENAI`: Represents the OpenAI language model.
     - `COHERE`: Represents the Cohere language model.
     - `GEMINI`: Represents the Gemini language model.
     - `DEEPSEEK`: Represents the DeepSeek language model.

2. **OpenAIEnums**:
   - Represents the different roles for OpenAI interactions.
   - Members:
     - `SYSTEM`: Represents the system role.
     - `USER`: Represents the user role.
     - `ASSISTANT`: Represents the assistant role.

3. **CohereEnums**:
   - Represents the different roles for Cohere interactions.
   - Members:
     - `SYSTEM`: Represents the system role.
     - `USER`: Represents the user role.
     - `ASSISTANT`: Represents the assistant role, referred to as "CHATBOT".
     - `DOCUMENT`: Represents a search document role.
     - `QUERY`: Represents a search query role.

4. **GeminiEnums**:
   - Represents the different roles for Google Gemini interactions.
   - Members:
     - `SYSTEM`: Represents the system role.
     - `USER`: Represents the user role.
     - `ASSISTANT`: Represents the assistant role, referred to as "model".
     - `DOCUMENT`: Represents a retrieval document role.
     - `QUERY`: Represents a retrieval query role.

5. **DocumentTypeEnum**:
   - Represents the type of document.
   - Members:
     - `DOCUMENT`: Represents a document type.
     - `QUERY`: Represents a query type.

6. **DeepSeekEnums**:
   - Represents the different roles for DeepSeek interactions.
   - Members:
     - `SYSTEM`: Represents the system role.
     - `USER`: Represents the user role.
     - `ASSISTANT`: Represents the assistant role.

### Notes:
- These enums help in maintaining consistency across different parts of the codebase when dealing with various roles and types associated with language model interactions.
- The naming conventions for roles and types are consistent with typical use cases for each platform, aiding in clarity and reducing errors in role/type assignment.
```
---

## File: stores/llm/LLMInterface.py

Summary:
```markdown
The `LLMInterface` class is an abstract base class designed to standardize the interface for language model providers, such as OpenAI or Cohere. It requires the implementation of several methods related to text generation and embedding, ensuring that any subclass adheres to a consistent API.

### Class: `LLMInterface`
- **Purpose**: To define a standard interface for language model interactions, including text generation and embedding functionalities.

### Methods

1. **`provider_name`** (property)
   - **Description**: Abstract property to get the provider's name as an enum value.
   - **Returns**: `LLMEnums` - The provider's enum value.

2. **`set_generation_model(model_id: str)`**
   - **Description**: Abstract method to set the generation model ID.
   - **Parameters**:
     - `model_id`: A string representing the ID of the generation model to use.

3. **`set_embedding_model(model_id: str, embedding_size: int) -> object`**
   - **Description**: Abstract method to set the embedding model ID and size.
   - **Parameters**:
     - `model_id`: A string representing the ID of the embedding model.
     - `embedding_size`: An integer specifying the size of the embedding.
   - **Returns**: An object representing the embedding model.

4. **`generate_text(prompt: str, chat_history: list, max_output_tokens: int, temperature: float = None)`**
   - **Description**: Abstract method to generate text based on a prompt and chat history.
   - **Parameters**:
     - `prompt`: A string containing the prompt for text generation.
     - `chat_history`: A list of previous conversation messages.
     - `max_output_tokens`: An integer for the maximum number of tokens to generate.
     - `temperature`: A float for the generation temperature (optional).
   - **Returns**: The generated text.

5. **`embed_text(text: str, document_type: str)`**
   - **Description**: Abstract method to embed text using the specified model.
   - **Parameters**:
     - `text`: A string of the text to be embedded.
     - `document_type`: A string indicating the type of document (e.g., 'document', 'query').
   - **Returns**: The generated embedding for the text.

6. **`construct_prompt(prompt: str, role: str)`**
   - **Description**: Abstract method to construct a prompt with a specified role.
   - **Parameters**:
     - `prompt`: A string of the prompt text.
     - `role`: A string representing the role of the sender (e.g., 'system', 'user').
   - **Returns**: A dictionary containing the role and processed prompt.

### Notes
- This interface ensures that any language model provider implementing it will have a consistent set of methods for setting models, generating text, embedding text, and constructing prompts.
- The `LLMEnums` is assumed to be an enumeration that defines possible provider names, which must be imported from another module.
```
---

## File: stores/llm/LLMProviderFactory.py

Summary:
```markdown
### Summary

The code defines a `LLMProviderFactory` class, which is a factory class responsible for creating instances of different Language Model (LLM) providers based on a specified provider name. It supports providers such as OpenAI, Cohere, Gemini, and DeepSeek. The factory uses a configuration dictionary to initialize these providers with necessary API keys and settings.

### Classes and Methods

- **LLMProviderFactory**: This is the main class that acts as a factory for creating LLM provider instances.

  - **`__init__(self, config: dict)`**: Initializes the factory with a configuration dictionary. The configuration should contain API keys and other settings required for the providers.

    - `config`: A dictionary containing configuration details like API keys and other settings.

  - **`create(self, provider: str)`**: Creates and returns an instance of the specified LLM provider based on the provided name.

    - `provider`: A string representing the name of the provider. It should match one of the values in `LLMEnums` (e.g., 'OPENAI', 'COHERE', 'GEMINI', 'DEEPSEEK').

    - Returns an instance of the corresponding provider class (e.g., `OpenAIProvider`, `CohereProvider`, `GeminiProvider`, `DeepSeekProvider`) or `None` if the provider is not recognized.

### Notes

- The `LLMEnums` class is used to standardize the provider names, ensuring consistency when specifying which provider to create.
- The factory method `create` checks the provider name against the `LLMEnums` values and initializes the corresponding provider with parameters extracted from the configuration dictionary.
- Each provider is initialized with specific parameters such as API keys, API URLs, and default settings for input and generation, which are expected to be present in the configuration dictionary.
- The code assumes the existence of classes `OpenAIProvider`, `CohereProvider`, `GeminiProvider`, and `DeepSeekProvider`, which are imported from their respective modules.
```
---

## File: stores/llm/__init__.py

Summary:
```markdown
The provided code is not visible. Please provide the code you would like documented, and I will assist you with a summary and documentation.
```
---

## File: stores/llm/providers/CoHereProvider.py

Summary:
```markdown
The `CohereProvider` class is a Python implementation that interfaces with the Cohere API for natural language processing tasks, such as text generation and embedding. It extends the `LLMInterface` and uses the `cohere` library to interact with the Cohere service. Below is a detailed breakdown of the class and its methods:

### Class: `CohereProvider`

#### Initialization
- **`__init__`**: Initializes the provider with API key and default parameters for text processing and generation.
  - `api_key`: API key for authenticating with the Cohere client.
  - `default_input_max_characters`: Maximum characters allowed per text input (default: 1000).
  - `default_generation_max_output_tokens`: Maximum tokens for text generation (default: 2000).
  - `default_generation_temperature`: Temperature for text generation (default: 0.1).

#### Properties
- **`provider_name`**: Returns the provider's enum value, `LLMEnums.COHERE`.

#### Methods
- **`set_generation_model`**: Sets the model ID for text generation.
  - `model_id`: The ID of the generation model.

- **`set_embedding_model`**: Sets the model ID and size for text embedding.
  - `model_id`: The ID of the embedding model.
  - `embedding_size`: The expected size of the embeddings.

- **`process_text`**: Trims the input text to the maximum allowed characters.
  - `text`: The text to process.

- **`generate_text`**: Generates text using the Cohere API.
  - `prompt`: The initial text prompt.
  - `chat_history`: A list of previous chat messages.
  - `max_output_tokens`: Maximum tokens for output.
  - `temperature`: Temperature for generation (optional, defaults to class setting).

- **`embed_text`**: Generates embeddings for the given text using the Cohere API.
  - `text`: The text to embed.
  - `document_type`: Type of document (e.g., query or document).

- **`construct_prompt`**: Constructs a prompt with an associated role.
  - `prompt`: The prompt text.
  - `role`: The role associated with the prompt.

### Notes
- The class requires a valid API key to initialize the Cohere client.
- It supports setting specific models for generation and embedding tasks.
- Error handling is implemented using logging to capture issues such as uninitialized clients or models.
- The `generate_text` and `embed_text` methods return `None` if there are errors during processing.
- The `process_text` method ensures that text inputs do not exceed the specified character limit.
```
---

## File: stores/llm/providers/DeepseekProvider.py

Summary:
```markdown
The `DeepSeekProvider` class is an implementation of the `LLMInterface` designed to interact with the DeepSeek API for text generation tasks. It does not support embeddings. Below is a detailed summary of its components:

### Class: `DeepSeekProvider`
- **Purpose**: Implements the `LLMInterface` to provide text generation capabilities using the DeepSeek API, with embeddings support disabled.
- **Initialization Parameters**:
  - `api_key`: API key for authentication. Defaults to the `DEEPSEEK_API_KEY` environment variable if not provided.
  - `api_url`: The endpoint URL for the DeepSeek API. Defaults to "https://api.deepseek.com/v1".
  - `default_input_max_characters`: The maximum number of characters allowed for input text. Default is 1000.
  - `default_generation_max_output_tokens`: The maximum number of tokens for generated output. Default is 2000.
  - `default_generation_temperature`: The temperature setting for text generation, affecting randomness. Default is 0.1.

### Properties
- **`provider_name`**: Returns the provider name from `LLMEnums`, specifically `LLMEnums.DEEPSEEK`.

### Methods
- **`set_generation_model(model_id: str)`**: Sets the model ID for text generation.
- **`set_embedding_model(model_id: str, embedding_size: int)`**: Raises `NotImplementedError` as DeepSeek does not support embeddings.
- **`process_text(text: str) -> str`**: Trims the input text to fit within the maximum allowed characters.
- **`generate_text(prompt: str, chat_history: list[dict], max_output_tokens: int = None, temperature: float = None) -> str | None`**:
  - Generates text based on the current conversation.
  - Uses the specified model ID and parameters for generation.
  - Logs errors if the model is not set or if there are issues with the API call.
- **`embed_text(text: str, document_type: str)`**: Raises `NotImplementedError` as DeepSeek does not support embeddings.
- **`construct_prompt(prompt: str, role: str) -> dict`**: Constructs a message structure with the given role and processed content.

### Notes
- The class uses the OpenAI client configured to communicate with the DeepSeek API.
- Logging is employed to capture errors and important events.
- The class assumes that the DeepSeek API is compatible with the OpenAI client interface for chat completions.
- The class is designed to be part of a larger system, as indicated by the use of enums and interfaces from other modules (`LLMEnums`, `OpenAIEnums`, `LLMInterface`).
```
---

## File: stores/llm/providers/GeminiProvider.py

Summary:
```markdown
The provided code defines a `GeminiProvider` class, which implements the `LLMInterface` using the Google Gemini (Generative AI) SDK. This class is designed to interact with Google's generative AI models for text generation and embedding tasks. Below is a detailed summary of the components and functionality:

### Class: `GeminiProvider`
- **Purpose**: Implements the `LLMInterface` to provide text generation and embedding functionalities using Google Gemini models.

#### Initialization
- **`__init__`**: 
  - Initializes the Gemini client with API key and configuration settings.
  - Parameters:
    - `api_key`: API key for Google Gemini.
    - `api_url`: URL for the API endpoint.
    - `default_input_max_characters`: Maximum characters for input text (default 1000).
    - `default_generation_max_output_tokens`: Maximum tokens for generated output (default 2000).
    - `default_generation_temperature`: Temperature setting for text generation (default 0.1).

#### Properties
- **`provider_name`**: Returns the provider name as `LLMEnums.GEMINI`.

#### Methods
- **`set_generation_model(model_id: str)`**: Sets the model ID for text generation.
- **`set_embedding_model(model_id: str, embedding_size: int)`**: Sets the model ID and expected size for embeddings.
- **`process_text(text: str) -> str`**: Trims input text to the maximum allowed length.
- **`generate_text(prompt: str, chat_history: list[dict], max_output_tokens: int = None, temperature: float = None) -> str`**:
  - Generates text based on a prompt and chat history.
  - Parameters:
    - `prompt`: The input text prompt.
    - `chat_history`: List of previous interactions.
    - `max_output_tokens`: Optional override for maximum output tokens.
    - `temperature`: Optional override for generation temperature.
- **`embed_text(text: str, document_type: str = None) -> list[float]`**:
  - Produces an embedding for the given text.
  - Parameters:
    - `text`: The text to be embedded.
    - `document_type`: Specifies if the text is a 'document' or 'query'.
- **`construct_prompt(prompt: str, role: str)`**:
  - Constructs a prompt with a specified role. Not actively used for Gemini as history is managed directly.

### Notes
- The class uses logging to report errors and warnings, such as when models are not set or when embedding sizes do not match expectations.
- The `embed_text` method includes error handling to log exceptions during embedding operations.
- The code includes commented-out sections for an alternative `embed_text` method, demonstrating a different approach to embedding.

This class is designed to facilitate interaction with Google's generative AI models, providing a structured way to configure and use text generation and embedding capabilities.
```
---

## File: stores/llm/providers/OpenAIProvider.py

Summary:
```markdown
The `OpenAIProvider` class is a wrapper around the OpenAI API, designed to facilitate text generation and embedding using OpenAI's models. It implements the `LLMInterface`, providing methods to configure and interact with the API.

### Class: `OpenAIProvider`

- **Initialization (`__init__`)**: 
  - Parameters:
    - `api_key`: API key for authenticating with OpenAI.
    - `api_url`: Optional base URL for the API.
    - `default_input_max_characters`: Maximum characters for input text (default is 1000).
    - `default_generation_max_output_tokens`: Maximum tokens for output generation (default is 2000).
    - `default_generation_temperature`: Temperature for text generation (default is 0.1).
  - Initializes the OpenAI client and sets default parameters for text processing and generation.

- **Properties**:
  - `provider_name`: Returns the provider enum `LLMEnums.OPENAI`.

- **Methods**:
  - `set_generation_model(model_id)`: Sets the model ID for text generation.
  - `set_embedding_model(model_id, embedding_size)`: Sets the model ID and size for text embedding.
  - `process_text(text)`: Trims the input text to the maximum allowed characters.
  - `generate_text(prompt, chat_history, max_output_tokens, temperature)`: Generates text using the OpenAI API. It constructs a prompt, appends it to the chat history, and requests a completion from the API.
    - Parameters:
      - `prompt`: The initial text prompt.
      - `chat_history`: List of previous messages for context.
      - `max_output_tokens`: Optional maximum tokens for the output.
      - `temperature`: Optional temperature for randomness in generation.
    - Returns the generated text or `None` if an error occurs.
  - `embed_text(text, document_type)`: Embeds text using the OpenAI API.
    - Parameters:
      - `text`: The text to embed.
      - `document_type`: Optional type of document (e.g., query or document).
    - Returns the embedding or `None` if an error occurs.
  - `construct_prompt(prompt, role)`: Constructs a prompt with a specified role (e.g., user or system).
    - Parameters:
      - `prompt`: The text of the prompt.
      - `role`: The role associated with the prompt.
    - Returns a dictionary containing the role and processed prompt.

### Notes
- The class requires an API key for OpenAI and optionally accepts a custom API URL.
- It logs errors if the client or models are not properly initialized.
- The class uses enums from `OpenAIEnums` and `LLMEnums` for role and provider identification.
- The `generate_text` and `embed_text` methods handle API responses and log errors if the responses are invalid or incomplete.
```
---

## File: stores/llm/providers/__init__.py

Summary:
```markdown
This code snippet imports two classes, `OpenAIProvider` and `CohereProvider`, from their respective modules within the same package. Here's a brief summary of each component:

1. **OpenAIProvider**:
   - This class is likely responsible for interfacing with the OpenAI API. It would typically include methods for sending requests to the API, handling responses, and possibly managing authentication and configuration settings specific to OpenAI's services.

2. **CohereProvider**:
   - This class is expected to serve a similar purpose as `OpenAIProvider`, but for the Cohere API. It would handle the specifics of communicating with Cohere's services, including sending requests, processing responses, and managing any necessary authentication or configuration.

**Notes for Future Reference**:
- Ensure that the modules `OpenAIProvider` and `CohereProvider` are correctly implemented and available in the package for these imports to work.
- These classes are likely part of a larger system designed to interact with multiple AI service providers, allowing for flexibility in choosing which AI service to use.
- Consider looking into the specific methods and attributes of each class to understand how they manage API interactions, including rate limiting, error handling, and data formatting.
```
---

## File: stores/templates/__init__.py

Summary:
```markdown
The provided code is not visible. Please provide the code you would like documented, and I will assist you with a detailed summary.
```
---

## File: stores/templates/locales/__init__.py

Summary:
```markdown
It seems that you've submitted an empty input. Please provide the code you would like documented, and I'll be happy to assist!
```
---

## File: stores/templates/locales/ar/__init__.py

Summary:
```markdown
The provided code is not visible. Please provide the code you would like documented.
```
---

## File: stores/templates/locales/ar/rag.py

Summary:
```markdown
This code defines a template-based system prompt for an AI assistant specialized in analyzing and answering questions about PDF documents. The system is designed to ensure that responses are accurate, based solely on the provided documents, and to handle various user requests effectively. Here's a breakdown of the components:

### Components:

1. **System Prompt (`system_prompt`)**:
   - A detailed set of instructions for the AI to follow when interacting with users.
   - Emphasizes the importance of providing answers only from the provided documents, acknowledging limitations, and avoiding hallucinations (i.e., making up information).
   - Includes guidelines for language preferences, response quality, conversation management, and handling technical documents.

2. **Document Prompt (`document_prompt`)**:
   - A template for referencing the source document and its content.
   - Ensures that the document content is treated as the primary source of truth.

3. **Evidence Format (`evidence_format`)**:
   - Provides a standardized way to cite information from documents, including document number and context.

4. **Context Awareness (`context_awareness`)**:
   - Instructions for understanding the full scope of available information and managing potential conflicts between documents.
   - Emphasizes the importance of considering the user's conversation history.

5. **Hallucination Prevention (`hallucination_prevention`)**:
   - Strategies to avoid making assumptions beyond the explicit content of the documents.
   - Encourages verification of every detail and clear marking of any inferences.

6. **Conversation Context Awareness (`conversation_context_awareness`)**:
   - Guidelines for intelligently handling questions when there is no information in the database.
   - Encourages leveraging conversation history to provide relevant responses.

7. **Language Handler (`language_handler`)**:
   - Instructions for detecting and responding to language-related requests, such as translation.
   - Ensures that translations maintain factual content and specialized terminology.

8. **Footer Prompt (`footer_prompt`)**:
   - Outlines the process for generating responses, including analyzing documents, identifying relevant information, and structuring responses appropriately.
   - Ensures that responses are based solely on the provided documents.

### Notes:
- The code uses Python's `string.Template` to create templates for various prompts.
- The system is designed to handle multilingual requests and provide accurate, document-based responses.
- The prompts emphasize critical thinking, accuracy, and the importance of context in generating responses.
```
---

## File: stores/templates/locales/en/__init__.py

Summary:
```markdown
The provided code snippet is empty. There are no functions, classes, or parameters to document. If you have a specific code snippet you would like documented, please provide it, and I will be happy to assist!
```
---

## File: stores/templates/locales/en/rag.py

Summary:
```markdown
This code defines a set of templates and guidelines for an AI assistant designed to answer questions based on PDF documents. The templates are used to generate prompts that guide the AI in providing accurate and relevant responses. Here's a breakdown of the components:

### Components:

1. **System Prompt**: 
   - **Purpose**: Guides the AI to answer questions based on provided PDF documents, ensuring responses are factual, precise, and derived solely from the documents.
   - **Key Features**:
     - Emphasizes using only document information.
     - Acknowledges limitations if information is missing.
     - Handles contradictions and prevents hallucinations.
     - Supports language translation and dialect variations.
     - Ensures high-quality, structured responses.

2. **Document Prompt**:
   - **Purpose**: Instructs the AI to treat the extracted document content as the primary source of truth.
   - **Structure**: Includes placeholders for document number and content.

3. **Evidence Citation Format**:
   - **Purpose**: Provides a format for citing specific document sections to support responses.
   - **Example**: "According to the financial projections [Doc 1, Q4 Forecast Table], revenue is expected to increase by 12%."

4. **Contextual Awareness Instructions**:
   - **Purpose**: Guides the AI to analyze all provided document chunks and consider user conversation history for context.

5. **Hallucination Prevention Instructions**:
   - **Purpose**: Ensures the AI verifies every detail against the documents to avoid making assumptions.

6. **Conversation Context Awareness**:
   - **Purpose**: Helps the AI handle questions related to previous conversations intelligently, using context to provide accurate responses.

7. **Language Request Handler**:
   - **Purpose**: Detects and handles language translation requests, ensuring factual content is preserved across translations.

8. **Footer Prompt**:
   - **Purpose**: Summarizes the response generation process, ensuring the AI analyzes documents thoroughly and structures responses appropriately.
   - **Includes**: Placeholders for user query and conversation context.

### Usage Notes:
- The AI is designed to prioritize document-based information, ensuring accuracy and preventing misinformation.
- It supports multilingual interactions, adapting to user language preferences.
- The templates guide the AI in maintaining a professional tone and structured responses.
- The system is equipped to handle complex document analysis, including technical content and multimodal data.

This setup is ideal for applications where precise, document-based responses are critical, such as legal, academic, or technical inquiries.
```
---

## File: stores/templates/summarization_templates/chunk_prompt_template.py

Summary:
```markdown
This code provides a comprehensive system for generating text summarization prompts in multiple languages, with a focus on maintaining high-quality standards. It includes several specialized templates for different types of content, such as technical, academic, legal, and medical texts, as well as a general template for multilingual support. Here's a breakdown of the key components:

### Templates

1. **chunk_prompt_template**: A basic template for summarizing text in Arabic, emphasizing the retention of main ideas, accuracy in numbers, and logical flow.

2. **enhanced_chunk_prompt_template**: An advanced template that includes additional context and language flexibility, allowing for a more nuanced summarization process.

3. **technical_chunk_prompt_template**: Designed for technical content, ensuring precision in technical terms, equations, and logical processes.

4. **specialized_chunk_prompt_template**: Tailored for specialized fields like legal or medical, focusing on accuracy in terminology and context.

5. **academic_chunk_prompt_template**: For academic and research texts, maintaining scientific rigor and detailed referencing.

6. **intelligent_auto_detect_template**: An intelligent template that analyzes text to determine its type and applies appropriate summarization criteria.

7. **MULTILINGUAL_CHUNK_TEMPLATES**: A dictionary of templates for different languages (Arabic, English, Spanish, French), each adhering to high-quality summarization standards.

### Functions

- **get_chunk_template(content_type, language, specialty)**: Selects the appropriate summarization template based on content type and language.

- **prepare_chunk_prompt(text, page_number, language, content_type, context)**: Prepares a prompt using the selected template, filling in variables such as text, page number, and context.

### Classes

- **ChunkPromptBuilder**: A class for building custom summarization prompts. It allows adding quality criteria and restrictions, and can generate a final template in either Arabic or English.

### Additional Templates

- **quality_check_template**: A template for reviewing and ensuring the quality of a summary, checking for accuracy, logical flow, and clarity.

### Notes

- The system is highly customizable, allowing for detailed control over the summarization process.
- It supports multiple languages and content types, making it versatile for various applications.
- The use of the `Template` class from Python's `string` module facilitates easy substitution of variables in the templates. 

This code is designed to be used in applications where precise and context-aware text summarization is required, such as academic research, technical documentation, or legal analysis.
```
---

## File: stores/templates/summarization_templates/merge_prompt_template.py

Summary:
```markdown
The provided code defines a `Template` from Python's `string` module, which is used to create a multilingual Markdown synthesis prompt. This prompt is designed for generating comprehensive and elegant Markdown documents without losing any information from the input summaries. The template is highly detailed and provides specific instructions for synthesizing documents in different target languages, including Arabic and English, with guidelines for other languages as well.

### Key Components:

- **Template Structure**: The template is a large string with placeholders (`$target_language` and `$summaries`) that can be filled with specific values when the template is used. It outlines a detailed process for creating a Markdown document that preserves all information from the input.

- **Language-Specific Instructions**:
  - **Arabic**: Use varied and sophisticated expressions, avoid repetition, and maintain right-to-left writing conventions.
  - **English**: Employ sophisticated transitions, use refined vocabulary, and prefer active voice for clarity.
  - **Other Languages**: Adapt style to linguistic conventions, maintain cultural sensitivity, and preserve all numerical data.

- **Preservation Rules**: Emphasizes the importance of preserving all numbers, dates, names, statistics, references, technical terms, quotes, and measurements exactly as they appear in the source material.

- **Synthesis Techniques**: Provides strategies for organizing and presenting information logically and elegantly, ensuring comprehensive coverage and seamless integration of concepts.

- **Markdown Structure**: Offers a detailed template for structuring the final Markdown document, including sections for executive synthesis, critical insights, comprehensive content analysis, conclusions, recommendations, and references.

- **Advanced Synthesis Principles**: Outlines principles for information architecture, redundancy management, preservation protocols, linguistic excellence, and quality assurance to ensure the final document is both complete and elegant.

### Usage:

- **`$target_language`**: Placeholder for specifying the language in which the document should be synthesized.
- **`$summaries`**: Placeholder for the input summaries that need to be transformed into the Markdown document.

This template is intended for use in scenarios where precise and comprehensive document synthesis is required, ensuring no information is lost while achieving a high level of elegance and professionalism in the final output.
```
---

## File: stores/templates/template_parser.py

Summary:
```markdown
The `TemplateParser` class is designed to manage and parse language templates for an application, enabling language switching and retrieval of localized strings. It operates by dynamically loading language-specific template modules and substituting variables within these templates.

### Class: `TemplateParser`

#### Initialization
- **`__init__(self, language: Optional[str] = None, default_language: str = 'en')`**
  - Initializes the parser with a specified or default language.
  - Parameters:
    - `language`: The language to use. Defaults to `None`, which falls back to the default language.
    - `default_language`: The default language to use if no specific language is set. Defaults to `'en'`.
  - Sets up the current path, default language, and initializes a cache for template groups.

#### Methods
- **`set_language(self, language: Optional[str]) -> bool`**
  - Sets the language for the templates.
  - Parameters:
    - `language`: The language to set.
  - Returns `True` if the language is set successfully, otherwise `False`.

- **`_load_module(self, group: str, language: str) -> Optional[Any]`**
  - Dynamically loads a module for a specific language and group.
  - Parameters:
    - `group`: The template group name.
    - `language`: The language code.
  - Returns the loaded module or `None` if not found.

- **`_get_template_group(self, group: str) -> Dict[str, Template]`**
  - Retrieves or loads a template group.
  - Parameters:
    - `group`: The group name.
  - Returns a dictionary of templates for the group.

- **`get(self, group: str, key: str, vars: Dict[str, str] = None) -> Optional[str]`**
  - Retrieves a localized string for a given group and key, substituting variables if provided.
  - Parameters:
    - `group`: The template group (e.g., 'errors', 'messages').
    - `key`: The specific key within the group.
    - `vars`: A dictionary of variables for substitution. Defaults to an empty dictionary.
  - Returns the localized string or `None` if not found.

- **`get_available_languages(self) -> list`**
  - Retrieves a list of all available languages.
  - Returns a list of language codes.

- **`get_current_language(self) -> str`**
  - Retrieves the currently set language.
  - Returns the current language code.

### Notes
- The class uses `os` for file path operations and `importlib` for dynamic module loading.
- Templates are expected to be Python files located in a `locales` directory structured by language codes.
- Template strings are managed using Python's `string.Template` class, allowing for variable substitution.
- The class includes a caching mechanism to optimize template loading and retrieval.
```
---

## File: stores/vectordb/VectorDBEnums.py

Summary:
```markdown
This code defines two enumerations using Python's `enum` module, which are used to represent different types of vector databases and distance methods for vector comparisons.

### Classes:

1. **VectorDBType (Enum)**
   - Represents different types of vector databases.
   - **Members:**
     - `QDRANT`: Represents the Qdrant vector database.
     - `FAISS`: Represents the FAISS vector database.
     - `ANNOY`: Represents the Annoy vector database.
     - `HNSWLIB`: Represents the HNSWLIB vector database.
     - `VESPA`: Represents the Vespa vector database.
     - `PINECONE`: Represents the Pinecone vector database.
     - `VECTORDB`: Represents a generic vector database.
     - `WEAVIATE`: Represents the Weaviate vector database.

2. **DistanceMethodEnum (Enum)**
   - Represents different methods for calculating distances between vectors.
   - **Members:**
     - `EUCLIDEAN`: Represents the Euclidean distance method.
     - `COSINE`: Represents the Cosine similarity method.
     - `DOT_PRODUCT`: Represents the Dot Product method.

### Notes:
- These enums are useful for standardizing the representation of vector database types and distance methods across a codebase.
- Enums provide a way to define a set of named values, which can be used to improve code readability and reduce errors associated with using string literals directly.
```
---

## File: stores/vectordb/VectorDBInterfase.py

Summary:
```markdown
The provided code defines an abstract base class `VectorDBInterfase` for interacting with vector databases. This class outlines the essential methods that any vector database provider (such as Qdrant, FAISS, or Weaviate) must implement to manage collections and perform CRUD operations with vectors. Below is a summary of the class and its methods:

### Class: `VectorDBInterfase`
- **Purpose**: Serves as an abstract base class for vector database interactions, ensuring a consistent interface for various vector database implementations.
- **Inheritance**: Inherits from `ABC` (Abstract Base Class) to enforce the implementation of abstract methods in subclasses.

### Methods

1. **`connect()`**
   - Establishes a connection to the vector database.
   - **Abstract Method**: Must be implemented by subclasses.

2. **`disconnect()`**
   - Disconnects from the vector database.
   - **Abstract Method**: Must be implemented by subclasses.

3. **`is_collection_existed(collection_name: str) -> bool`**
   - Checks if a collection exists in the vector database.
   - **Parameters**:
     - `collection_name`: The name of the collection to check.
   - **Returns**: `True` if the collection exists, `False` otherwise.
   - **Abstract Method**: Must be implemented by subclasses.

4. **`list_all_collections() -> List`**
   - Lists all collections in the vector database.
   - **Returns**: A list of collection names.
   - **Abstract Method**: Must be implemented by subclasses.

5. **`get_collection_info(collection_name: str) -> dict`**
   - Retrieves information about a specific collection.
   - **Parameters**:
     - `collection_name`: The name of the collection to get information for.
   - **Returns**: A dictionary containing collection information.
   - **Abstract Method**: Must be implemented by subclasses.

6. **`delete_collection(collection_name: str)`**
   - Deletes a collection from the vector database.
   - **Parameters**:
     - `collection_name`: The name of the collection to delete.
   - **Abstract Method**: Must be implemented by subclasses.

7. **`create_collection(collection_name: str, embedding_size: int, do_reset: bool = False)`**
   - Creates a new collection in the vector database.
   - **Parameters**:
     - `collection_name`: The name of the collection to create.
     - `embedding_size`: The size of the vectors to store in the collection.
     - `do_reset`: Whether to reset the collection if it already exists (default: `False`).
   - **Abstract Method**: Must be implemented by subclasses.

8. **`insert_one(collection_name: str, text: str, vector: list, metadata: dict = None, record_id: str = None)`**
   - Inserts a single document into a collection.
   - **Parameters**:
     - `collection_name`: The name of the collection.
     - `text`: The text associated with the vector.
     - `vector`: The vector to insert.
     - `metadata`: Optional metadata to associate with the document.
     - `record_id`: Optional ID for the document.
   - **Returns**: `True` if insertion was successful, `False` otherwise.
   - **Abstract Method**: Must be implemented by subclasses.

9. **`insert_many(collection_name: str, texts: list, vectors: list, metadata: List[dict] = None, record_ids: List[str] = None, batch_size: int = 50)`**
   - Inserts multiple documents into a collection.
   - **Parameters**:
     - `collection_name`: The name of the collection.
     - `texts`: A list of texts to associate with the vectors.
     - `vectors`: A list of vectors to insert.
     - `metadata`: Optional list of metadata to associate with the documents.
     - `record_ids`: Optional list of IDs for the documents.
     - `batch_size`: The size of the batches for insertion (default: 50).
   - **Returns**: `True` if all documents were inserted successfully, `False` otherwise.
   - **Abstract Method**: Must be implemented by subclasses.

10. **`search_by_vector(collection_name: str, vector: list, limit: int) -> List[RetrieveDocument]`**
    - Searches for documents in a collection by vector.
    - **Parameters**:
      - `collection_name`: The name of the collection to search in.
      - `vector`: The query vector.
      - `limit`: The maximum number of results to return.
    - **Returns**: A list of retrieved documents or an empty list if no results are found.
    - **Abstract Method**: Must be implemented by subclasses.

### Notes
- The class uses Python's `abc` module to define abstract methods, ensuring that any subclass must implement these methods.
- The `RetrieveDocument` type hint suggests that the `search_by_vector` method returns a list of objects of this type, which is likely defined elsewhere in the `models.db_schemes` module.
- This interface provides a standardized way to interact with vector databases, facilitating the implementation of different backend providers.
```
---

## File: stores/vectordb/VectorDBProviderFactory.py

Summary:
```markdown
The provided code defines a `VectorDBProviderFactory` class, which is responsible for creating instances of different vector database providers, such as Qdrant and Weaviate. The factory uses a configuration object to initialize the appropriate provider based on the specified type.

### Class: `VectorDBProviderFactory`
- **Purpose**: A factory class to instantiate vector database providers based on a given configuration.
- **Initialization**: 
  - `__init__(self, config)`: Initializes the factory with a configuration object. The configuration should contain settings like database paths and distance methods. It also initializes a `BaseController` instance.

### Method: `create`
- **Purpose**: To create and return an instance of a specified vector database provider.
- **Parameters**:
  - `provider (str)`: The name of the vector database provider to instantiate (e.g., 'QDRANT', 'WEAVIATE').
- **Returns**: An instance of the specified provider class (`QdrantDBProvider` or `WeaviateDBProvider`), or `None` if the provider is not supported.
- **Functionality**:
  - For `QDRANT`: Uses the `BaseController` to get the database path and initializes a `QdrantDBProvider` with the path and distance method.
  - For `WEAVIATE`: Initializes a `WeaviateDBProvider` with the database URL, distance method, and optionally an API key.
  - If the provider is not recognized, it returns `None`.

### Notes
- The factory relies on a `VectorDBType` enumeration to determine the provider type.
- The configuration object (`config`) must have attributes like `VECTOR_DB_PATH` and `VECTOR_DB_DISTANCE_METHOD`.
- The `WeaviateDBProvider` initialization is flexible with the API key, using `getattr` to avoid errors if the key is not defined in the configuration.
```
---

## File: stores/vectordb/__init__.py

Summary:
```markdown
It seems that there is no code provided for documentation. Please provide the code you would like summarized, and I will be happy to assist you.
```
---

## File: stores/vectordb/providers/MilvusDBProvider.py

Summary:
```markdown
The provided code defines a class `MilvusDBProvider` that serves as an interface for interacting with a Milvus vector database. It extends a base class `VectorDBInterfase` and provides methods for managing collections and performing operations such as connecting to the database, inserting data, and searching by vector similarity.

### Class: `MilvusDBProvider`
- **Purpose**: To manage and interact with Milvus collections for storing and retrieving vector data.
- **Constructor Parameters**:
  - `host` (str): The host address of the Milvus server.
  - `port` (str): The port number for the Milvus server.
  - `collection_prefix` (str, optional): A prefix for collection names to avoid conflicts.

### Methods

- **`_get_full_collection_name(collection_name: str) -> str`**: 
  - Returns the full collection name by appending the prefix to the given collection name.

- **`connect()`**: 
  - Establishes a connection to the Milvus server. Logs success or failure.

- **`disconnect()`**: 
  - Disconnects from the Milvus server. Logs success or failure.

- **`is_collection_existed(collection_name: str) -> bool`**: 
  - Checks if a collection exists in the database.

- **`list_all_collections() -> List`**: 
  - Returns a list of all collections in the database.

- **`get_collection_info(collection_name: str) -> dict`**: 
  - Retrieves and returns information about a specified collection.

- **`delete_collection(collection_name: str)`**: 
  - Deletes a specified collection if it exists.

- **`create_collection(collection_name: str, embedding_size: int, do_reset: bool = False)`**: 
  - Creates a new collection with specified fields and schema. Optionally resets the collection if it already exists.

- **`insert_one(collection_name: str, text: str, vector: list, metadata: dict = None, record_id: str = None)`**: 
  - Inserts a single record into a collection. Converts metadata to a JSON string if provided.

- **`insert_many(collection_name: str, texts: list, vectors: list, metadata: List[dict] = None, record_ids: List[str] = None, batch_size: int = 50)`**: 
  - Inserts multiple records in batches. Handles metadata conversion and logs batch insertions.

- **`search_by_vector(collection_name: str, vector: list, limit: int = 5) -> List[RetrieveDocument]`**: 
  - Performs a vector similarity search in a collection and returns a list of `RetrieveDocument` objects containing the search results.

### Notes
- The code uses the `pymilvus` library for interacting with Milvus.
- Logging is extensively used to track operations and errors.
- JSON is used for metadata serialization.
- The `RetrieveDocument` class is assumed to be defined elsewhere, representing the structure of search results.
- The code includes Arabic comments, indicating the intended audience or developers might be Arabic-speaking.
```
---

## File: stores/vectordb/providers/QdrantDBProvider.py

Summary:
```markdown
The `QdrantDBProvider` class is designed to interact with a Qdrant database, which is a vector database. This class extends the `VectorDBInterfase` and provides methods for managing collections and performing operations such as inserting, deleting, and searching documents using vectors.

### Class: `QdrantDBProvider`

- **Purpose**: To manage and interact with a Qdrant vector database.
- **Initialization**:
  - `db_path` (str): Path to the Qdrant database.
  - `distance_method` (str): Method for vector distance calculation, either 'COSINE' or 'DOT_PRODUCT'.

### Methods

- **`__init__(self, db_path: str, distance_method: str)`**: 
  - Initializes the database client and sets the distance method based on the provided enum value.

- **`connect(self)`**: 
  - Establishes a connection to the Qdrant client using the specified database path.

- **`disconnect(self)`**: 
  - Disconnects the Qdrant client.

- **`is_collection_existed(self, collection_name: str) -> bool`**: 
  - Checks if a specific collection exists in the database.

- **`list_all_collections(self) -> List`**: 
  - Returns a list of all collection names in the database.

- **`get_collection_info(self, collection_name: str) -> dict`**: 
  - Retrieves information about a specific collection.

- **`delete_collection(self, collection_name: str)`**: 
  - Deletes a collection if it exists.

- **`create_collection(self, collection_name: str, embedding_size: int, do_reset: bool = False)`**: 
  - Creates a new collection or resets an existing one with specified parameters.
  - `embedding_size` (int): Size of the vectors.
  - `do_reset` (bool): If true, resets the collection if it exists.

- **`insert_one(self, collection_name: str, text: str, vector: list, metadata: dict = None, record_id: str = None)`**: 
  - Inserts a single record into a collection.
  - `text` (str): Text associated with the vector.
  - `vector` (list): Vector to insert.
  - `metadata` (dict, optional): Additional metadata.
  - `record_id` (str, optional): ID for the record.

- **`insert_many(self, collection_name: str, texts: list, vectors: list, metadata: list = None, record_ids: list = None, embedding_size: int = 1536, batch_size: int = 50)`**: 
  - Inserts multiple records in batches.
  - `texts`, `vectors`, `metadata`, `record_ids`: Lists of data to insert.
  - `embedding_size` (int): Expected size of vectors.
  - `batch_size` (int): Number of records per batch.

- **`search_by_vector(self, collection_name: str, vector: list, limit: int = 5)`**: 
  - Searches for documents based on a query vector.
  - `vector` (list): Query vector.
  - `limit` (int): Max number of results to return.

### Notes

- The class uses the `QdrantClient` for database operations and `models` from the `qdrant_client` package for defining vector parameters and point structures.
- Logging is used extensively to track operations and errors.
- The class assumes that the `DistanceMethodEnum` and `RetrieveDocument` are defined elsewhere in the codebase.
- Proper exception handling is implemented to ensure robustness during database operations.
```
---

## File: stores/vectordb/providers/WeaviateDBProvider.py

Summary:
```markdown
The provided code defines a class `WeaviateDBProvider` which implements a vector database interface for interacting with a Weaviate database. This class is designed to manage collections and perform operations such as connecting to the database, creating collections, inserting documents, and searching for documents using vector-based retrieval.

### Class: `WeaviateDBProvider`

- **Initialization**: 
  - `__init__(self, db_url: str, distance_method: str, api_key: str = None)`: Initializes the provider with a database URL, a distance method (cosine, dot product, or Euclidean), and an optional API key.

- **Connection Management**:
  - `connect(self)`: Establishes a connection to the Weaviate database and checks if the connection is successful.
  - `disconnect(self)`: Disconnects from the Weaviate database.

- **Collection Management**:
  - `is_collection_existed(self, collection_name: str) -> bool`: Checks if a collection exists in the database.
  - `list_all_collections(self) -> List`: Lists all collections in the database.
  - `get_collection_info(self, collection_name: str) -> dict`: Retrieves information about a specific collection.
  - `delete_collection(self, collection_name: str)`: Deletes a specified collection.
  - `create_collection(self, collection_name: str, embedding_size: int, do_reset: bool = False)`: Creates a new collection with specified properties, optionally resetting it if it already exists.

- **Data Insertion**:
  - `insert_one(self, collection_name: str, text: str, vector: list, metadata: dict = None, record_id: str = None)`: Inserts a single document into a collection.
  - `insert_many(self, collection_name: str, texts: List[str], vectors: List[list], metadata: Optional[List[dict]] = None, record_ids: Optional[List[str]] = None, batch_size: int = 50) -> bool`: Inserts multiple documents in batches, ensuring all input lists have the same length.

- **Search Functionality**:
  - `search_by_vector(self, collection_name: str, vector: List[float], query_text: Optional[str] = None, limit: int = 5, min_score: float = 0.3, use_hybrid: bool = True) -> List[RetrieveDocument]`: Searches for documents using a vector, with optional hybrid search combining text and vector queries. It returns a list of `RetrieveDocument` objects or a default response if no results are found.

### Notes:
- The class uses the `weaviate` library for database operations and supports advanced search features like hybrid search and semantic expansion.
- The `insert_many` function ensures input data integrity by checking the lengths of input lists and validating UUIDs.
- The `search_by_vector` function includes error handling and retries using the `tenacity` library for robust query execution.
- The code includes placeholder comments for additional search enhancements and advanced text processing, indicating areas for further development or customization.

This class is suitable for applications requiring efficient vector-based retrieval from a Weaviate database, such as document search engines or AI-driven content recommendation systems.
```
---

## File: stores/vectordb/providers/__init__.py

Summary:
```markdown
The provided code is not visible. Please provide the code you would like summarized or documented.
```
---

## File: websocket_handlers.py

Summary:
```markdown
The code defines a WebSocket connection manager and related functions for handling WebSocket connections in a FastAPI application. It facilitates real-time communication between the server and clients, specifically for managing asset-related events and updates.

### Classes

- **ConnectionManager**: Manages WebSocket connections, storing active connections by asset ID and handling connection and disconnection logic.
  - **`__init__`**: Initializes dictionaries to track connections by asset ID and WebSocket.
  - **`connect`**: Accepts a new WebSocket connection, associates it with an asset ID, and sends a confirmation message.
  - **`disconnect`**: Removes a WebSocket from the active connections and logs the disconnection.
  - **`send_personal_message`**: Sends a JSON message to a specific WebSocket.
  - **`broadcast_to_asset`**: Sends a message to all WebSockets associated with a specific asset ID.
  - **`broadcast_to_all`**: Sends a message to all connected WebSockets.

### Helper Functions

These functions broadcast specific events related to file processing and indexing to WebSocket clients:
- **`emit_file_processing_started`**: Notifies clients that file processing has started.
- **`emit_file_processing_progress`**: Updates clients on the progress of file processing.
- **`emit_file_processing_completed`**: Informs clients that file processing is complete.
- **`emit_indexing_started`**: Notifies clients that indexing has started.
- **`emit_indexing_progress`**: Updates clients on the progress of indexing.
- **`emit_indexing_completed`**: Informs clients that indexing is complete.
- **`emit_error`**: Sends an error message to clients.
- **`emit_rag_answer_started`**, **`emit_rag_answer_stream`**, **`emit_rag_answer_completed`**: Handle events related to RAG (Retrieval-Augmented Generation) answers.

### Setup Function

- **`setup_websocket_handlers`**: Configures the FastAPI application to handle WebSocket connections and events.
  - Initializes a `ConnectionManager`.
  - Adds event-emitting functions to the FastAPI app.
  - Defines a WebSocket endpoint (`/socket.io`) to manage connections, subscriptions, and handle incoming messages.

### Notes
- The WebSocket endpoint listens for specific events like "subscribe", "unsubscribe", and "ping", and responds accordingly.
- The code includes error handling for JSON decoding errors and WebSocket disconnections.
- Logging is used extensively to track connection status and errors.
```
---
